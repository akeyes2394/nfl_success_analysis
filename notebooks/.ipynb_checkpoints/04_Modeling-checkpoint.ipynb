{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv('./data/modeling_data')\n",
    "players.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    182\n",
       "2017    176\n",
       "2016    171\n",
       "2015    158\n",
       "2013    135\n",
       "2012    133\n",
       "2011    131\n",
       "2014    125\n",
       "2010    125\n",
       "2009    124\n",
       "Name: draft_year, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players['draft_year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = players[players['draft_year']<=2015].drop(columns=['avg_grade', 'draft_year', 'player', 'position', 'school', 'conference', 'team_nfl'])\n",
    "y_train = players[players['draft_year']<=2015]['avg_grade']\n",
    "X_test = players[players['draft_year']>2015].drop(columns=['avg_grade', 'draft_year', 'player', 'position', 'school', 'conference', 'team_nfl'])\n",
    "y_test = players[players['draft_year']>2015]['avg_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6376712328767123"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) /  len(players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>draft_overall</th>\n",
       "      <th>draft_round</th>\n",
       "      <th>avg_pass_cmp</th>\n",
       "      <th>avg_pass_att</th>\n",
       "      <th>avg_pass_cmp_pct</th>\n",
       "      <th>avg_pass_yds</th>\n",
       "      <th>avg_pass_yds_per_att</th>\n",
       "      <th>avg_adj_pass_yds_per_att</th>\n",
       "      <th>avg_pass_td</th>\n",
       "      <th>avg_pass_int</th>\n",
       "      <th>...</th>\n",
       "      <th>final_year_fumbles_rec_td</th>\n",
       "      <th>final_year_fumbles_forced</th>\n",
       "      <th>final_year_punt_ret</th>\n",
       "      <th>final_year_punt_ret_yds</th>\n",
       "      <th>final_year_punt_ret_yds_per_ret</th>\n",
       "      <th>final_year_punt_ret_td</th>\n",
       "      <th>final_year_kick_ret</th>\n",
       "      <th>final_year_kick_ret_yds</th>\n",
       "      <th>final_year_kick_ret_yds_per_ret</th>\n",
       "      <th>final_year_kick_ret_td</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>425.5</td>\n",
       "      <td>66.100000</td>\n",
       "      <td>3982.000000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>9.6</td>\n",
       "      <td>32.5</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>259.666667</td>\n",
       "      <td>389.0</td>\n",
       "      <td>66.766667</td>\n",
       "      <td>3598.666667</td>\n",
       "      <td>9.166667</td>\n",
       "      <td>10.4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     draft_overall  draft_round  avg_pass_cmp  avg_pass_att  avg_pass_cmp_pct  \\\n",
       "529              1            1    281.000000         425.5         66.100000   \n",
       "530              2            1    259.666667         389.0         66.766667   \n",
       "531              4            1      0.000000           0.0          0.000000   \n",
       "532              6            1      0.000000           0.0          0.000000   \n",
       "533              7            1      0.000000           0.0          0.000000   \n",
       "\n",
       "     avg_pass_yds  avg_pass_yds_per_att  avg_adj_pass_yds_per_att  \\\n",
       "529   3982.000000              9.500000                       9.6   \n",
       "530   3598.666667              9.166667                      10.4   \n",
       "531      0.000000              0.000000                       0.0   \n",
       "532      0.000000              0.000000                       0.0   \n",
       "533      0.000000              0.000000                       0.0   \n",
       "\n",
       "     avg_pass_td  avg_pass_int           ...            \\\n",
       "529         32.5     14.000000           ...             \n",
       "530         35.0      4.666667           ...             \n",
       "531          0.0      0.000000           ...             \n",
       "532          0.0      0.000000           ...             \n",
       "533          0.0      0.000000           ...             \n",
       "\n",
       "     final_year_fumbles_rec_td  final_year_fumbles_forced  \\\n",
       "529                        0.0                        1.0   \n",
       "530                        0.0                        0.0   \n",
       "531                        0.0                        0.0   \n",
       "532                        0.0                        3.0   \n",
       "533                        0.0                        0.0   \n",
       "\n",
       "     final_year_punt_ret  final_year_punt_ret_yds  \\\n",
       "529                  0.0                      0.0   \n",
       "530                  0.0                      0.0   \n",
       "531                  0.0                      0.0   \n",
       "532                  0.0                      0.0   \n",
       "533                  0.0                      0.0   \n",
       "\n",
       "     final_year_punt_ret_yds_per_ret  final_year_punt_ret_td  \\\n",
       "529                              0.0                     0.0   \n",
       "530                              0.0                     0.0   \n",
       "531                              0.0                     0.0   \n",
       "532                              0.0                     0.0   \n",
       "533                              0.0                     0.0   \n",
       "\n",
       "     final_year_kick_ret  final_year_kick_ret_yds  \\\n",
       "529                  0.0                      0.0   \n",
       "530                  0.0                      0.0   \n",
       "531                  0.0                      0.0   \n",
       "532                  0.0                      0.0   \n",
       "533                  0.0                      0.0   \n",
       "\n",
       "     final_year_kick_ret_yds_per_ret  final_year_kick_ret_td  \n",
       "529                              0.0                     0.0  \n",
       "530                              0.0                     0.0  \n",
       "531                              0.0                     0.0  \n",
       "532                              0.0                     0.0  \n",
       "533                              0.0                     0.0  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14842356260905676"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729.4138376825765"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19047769076753462"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693.3925697568789"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = players[players['draft_year']<=2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school'])\n",
    "y_train = players[players['draft_year']<=2015]['avg_grade']\n",
    "X_test = players[players['draft_year']>2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school'])\n",
    "y_test = players[players['draft_year']>2015]['avg_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>draft_overall</th>\n",
       "      <th>draft_round</th>\n",
       "      <th>position</th>\n",
       "      <th>conference</th>\n",
       "      <th>team_nfl</th>\n",
       "      <th>avg_pass_cmp</th>\n",
       "      <th>avg_pass_att</th>\n",
       "      <th>avg_pass_cmp_pct</th>\n",
       "      <th>avg_pass_yds</th>\n",
       "      <th>avg_pass_yds_per_att</th>\n",
       "      <th>...</th>\n",
       "      <th>final_year_fumbles_rec_td</th>\n",
       "      <th>final_year_fumbles_forced</th>\n",
       "      <th>final_year_punt_ret</th>\n",
       "      <th>final_year_punt_ret_yds</th>\n",
       "      <th>final_year_punt_ret_yds_per_ret</th>\n",
       "      <th>final_year_punt_ret_td</th>\n",
       "      <th>final_year_kick_ret</th>\n",
       "      <th>final_year_kick_ret_yds</th>\n",
       "      <th>final_year_kick_ret_yds_per_ret</th>\n",
       "      <th>final_year_kick_ret_td</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>QB</td>\n",
       "      <td>ACC</td>\n",
       "      <td>TAM</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>425.5</td>\n",
       "      <td>66.100000</td>\n",
       "      <td>3982.000000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>QB</td>\n",
       "      <td>Pac-12</td>\n",
       "      <td>TEN</td>\n",
       "      <td>259.666667</td>\n",
       "      <td>389.0</td>\n",
       "      <td>66.766667</td>\n",
       "      <td>3598.666667</td>\n",
       "      <td>9.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>WR</td>\n",
       "      <td>SEC</td>\n",
       "      <td>OAK</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>DE</td>\n",
       "      <td>Pac-12</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>WR</td>\n",
       "      <td>Big 12</td>\n",
       "      <td>CHI</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     draft_overall  draft_round position conference team_nfl  avg_pass_cmp  \\\n",
       "529              1            1       QB        ACC      TAM    281.000000   \n",
       "530              2            1       QB     Pac-12      TEN    259.666667   \n",
       "531              4            1       WR        SEC      OAK      0.000000   \n",
       "532              6            1       DE     Pac-12      NYJ      0.000000   \n",
       "533              7            1       WR     Big 12      CHI      0.000000   \n",
       "\n",
       "     avg_pass_att  avg_pass_cmp_pct  avg_pass_yds  avg_pass_yds_per_att  \\\n",
       "529         425.5         66.100000   3982.000000              9.500000   \n",
       "530         389.0         66.766667   3598.666667              9.166667   \n",
       "531           0.0          0.000000      0.000000              0.000000   \n",
       "532           0.0          0.000000      0.000000              0.000000   \n",
       "533           0.0          0.000000      0.000000              0.000000   \n",
       "\n",
       "              ...            final_year_fumbles_rec_td  \\\n",
       "529           ...                                  0.0   \n",
       "530           ...                                  0.0   \n",
       "531           ...                                  0.0   \n",
       "532           ...                                  0.0   \n",
       "533           ...                                  0.0   \n",
       "\n",
       "     final_year_fumbles_forced  final_year_punt_ret  final_year_punt_ret_yds  \\\n",
       "529                        1.0                  0.0                      0.0   \n",
       "530                        0.0                  0.0                      0.0   \n",
       "531                        0.0                  0.0                      0.0   \n",
       "532                        3.0                  0.0                      0.0   \n",
       "533                        0.0                  0.0                      0.0   \n",
       "\n",
       "     final_year_punt_ret_yds_per_ret  final_year_punt_ret_td  \\\n",
       "529                              0.0                     0.0   \n",
       "530                              0.0                     0.0   \n",
       "531                              0.0                     0.0   \n",
       "532                              0.0                     0.0   \n",
       "533                              0.0                     0.0   \n",
       "\n",
       "     final_year_kick_ret  final_year_kick_ret_yds  \\\n",
       "529                  0.0                      0.0   \n",
       "530                  0.0                      0.0   \n",
       "531                  0.0                      0.0   \n",
       "532                  0.0                      0.0   \n",
       "533                  0.0                      0.0   \n",
       "\n",
       "     final_year_kick_ret_yds_per_ret  final_year_kick_ret_td  \n",
       "529                              0.0                     0.0  \n",
       "530                              0.0                     0.0  \n",
       "531                              0.0                     0.0  \n",
       "532                              0.0                     0.0  \n",
       "533                              0.0                     0.0  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns = ['position', 'conference', 'team_nfl'], drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns = ['position', 'conference', 'team_nfl'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 144)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_DB\n",
      "conference_Big East\n",
      "conference_WAC\n",
      "team_nfl_STL\n"
     ]
    }
   ],
   "source": [
    "for i in X_train.columns:\n",
    "    if i not in X_test.columns:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conference_MVC\n",
      "team_nfl_LAC\n",
      "team_nfl_LAR\n"
     ]
    }
   ],
   "source": [
    "for i in X_test.columns:\n",
    "    if i not in X_train.columns:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['conference_MVC'] = 0\n",
    "X_train['team_nfl_LAC'] = 0\n",
    "X_train['team_nfl_LAR'] = 0\n",
    "\n",
    "X_test['position_DB'] = 0\n",
    "X_test['conference_Big East'] = 0\n",
    "X_test['conference_WAC'] = 0\n",
    "X_test['team_nfl_STL'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2670286304434637"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1085.2674818211156"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)\n",
    "rf_preds = rf_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21989293688389433"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668.1970774745326"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 147)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[1], input_shape=(147,), activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 931 samples, validate on 529 samples\n",
      "Epoch 1/30\n",
      "931/931 [==============================] - 1s 546us/step - loss: 6247.7985 - val_loss: 1152.6713\n",
      "Epoch 2/30\n",
      "931/931 [==============================] - 0s 45us/step - loss: 1013.1504 - val_loss: 1046.8429\n",
      "Epoch 3/30\n",
      "931/931 [==============================] - 0s 48us/step - loss: 891.7886 - val_loss: 1034.0449\n",
      "Epoch 4/30\n",
      "931/931 [==============================] - 0s 44us/step - loss: 847.8585 - val_loss: 1066.8266\n",
      "Epoch 5/30\n",
      "931/931 [==============================] - 0s 46us/step - loss: 843.4260 - val_loss: 1132.9507\n",
      "Epoch 6/30\n",
      "931/931 [==============================] - 0s 45us/step - loss: 790.5907 - val_loss: 978.6102\n",
      "Epoch 7/30\n",
      "931/931 [==============================] - 0s 45us/step - loss: 782.5406 - val_loss: 1069.9246\n",
      "Epoch 8/30\n",
      "931/931 [==============================] - 0s 44us/step - loss: 802.0811 - val_loss: 1115.2465\n",
      "Epoch 9/30\n",
      "931/931 [==============================] - 0s 44us/step - loss: 771.8011 - val_loss: 991.6461\n",
      "Epoch 10/30\n",
      "931/931 [==============================] - 0s 46us/step - loss: 750.0332 - val_loss: 1168.6238\n",
      "Epoch 11/30\n",
      "931/931 [==============================] - 0s 48us/step - loss: 773.5288 - val_loss: 1024.7318\n",
      "Epoch 12/30\n",
      "931/931 [==============================] - 0s 47us/step - loss: 717.4021 - val_loss: 1171.3876\n",
      "Epoch 13/30\n",
      "931/931 [==============================] - 0s 43us/step - loss: 749.0704 - val_loss: 1016.1845\n",
      "Epoch 14/30\n",
      "931/931 [==============================] - 0s 45us/step - loss: 697.2047 - val_loss: 1067.1892\n",
      "Epoch 15/30\n",
      "931/931 [==============================] - 0s 46us/step - loss: 674.9925 - val_loss: 1017.7363\n",
      "Epoch 16/30\n",
      "931/931 [==============================] - 0s 44us/step - loss: 755.0862 - val_loss: 1087.9464\n",
      "Epoch 17/30\n",
      "931/931 [==============================] - 0s 45us/step - loss: 698.4697 - val_loss: 996.7139\n",
      "Epoch 18/30\n",
      "931/931 [==============================] - 0s 44us/step - loss: 694.6508 - val_loss: 1011.6554\n",
      "Epoch 19/30\n",
      "931/931 [==============================] - 0s 48us/step - loss: 711.7858 - val_loss: 1091.7911\n",
      "Epoch 20/30\n",
      "931/931 [==============================] - 0s 43us/step - loss: 657.9514 - val_loss: 1071.7667\n",
      "Epoch 21/30\n",
      "931/931 [==============================] - 0s 45us/step - loss: 622.5088 - val_loss: 1009.6585\n",
      "Epoch 22/30\n",
      "931/931 [==============================] - 0s 43us/step - loss: 632.8646 - val_loss: 1126.5845\n",
      "Epoch 23/30\n",
      "931/931 [==============================] - 0s 44us/step - loss: 647.6289 - val_loss: 1100.6101\n",
      "Epoch 24/30\n",
      "931/931 [==============================] - 0s 46us/step - loss: 659.0978 - val_loss: 1255.0405\n",
      "Epoch 25/30\n",
      "931/931 [==============================] - 0s 43us/step - loss: 626.7623 - val_loss: 1066.6564\n",
      "Epoch 26/30\n",
      "931/931 [==============================] - 0s 44us/step - loss: 619.8207 - val_loss: 962.5449\n",
      "Epoch 27/30\n",
      "931/931 [==============================] - 0s 43us/step - loss: 667.2112 - val_loss: 1111.3230\n",
      "Epoch 28/30\n",
      "931/931 [==============================] - 0s 51us/step - loss: 609.8951 - val_loss: 904.0284\n",
      "Epoch 29/30\n",
      "931/931 [==============================] - 0s 45us/step - loss: 654.9715 - val_loss: 1055.6276\n",
      "Epoch 30/30\n",
      "931/931 [==============================] - 0s 44us/step - loss: 567.8079 - val_loss: 1034.4062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d5c202c160>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20764908363753376"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming Features and Models by Position\n",
    "\n",
    "Models are performing poorly with all of the almost 150 columns included in training.  I am going to try using only the average metrics from college rather than the final_year metrics as the averages looked to be more consistiently and better spread in the EDA visuals.  \n",
    "\n",
    "I will also split the data by position and create models for each individually to see how they perform.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_year_cols = [i for i in players.columns if 'final_year' in i]\n",
    "avg_cols = [i for i in players.columns if i not in final_year_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['draft_overall',\n",
       " 'draft_round',\n",
       " 'draft_year',\n",
       " 'player',\n",
       " 'position',\n",
       " 'school',\n",
       " 'conference',\n",
       " 'team_nfl',\n",
       " 'avg_grade',\n",
       " 'avg_pass_cmp',\n",
       " 'avg_pass_att',\n",
       " 'avg_pass_cmp_pct',\n",
       " 'avg_pass_yds',\n",
       " 'avg_pass_yds_per_att',\n",
       " 'avg_adj_pass_yds_per_att',\n",
       " 'avg_pass_td',\n",
       " 'avg_pass_int',\n",
       " 'avg_pass_rating',\n",
       " 'avg_rush_att',\n",
       " 'avg_rush_yds',\n",
       " 'avg_rush_yds_per_att',\n",
       " 'avg_rush_td',\n",
       " 'avg_rec',\n",
       " 'avg_rec_yds',\n",
       " 'avg_rec_yds_per_rec',\n",
       " 'avg_rec_td',\n",
       " 'avg_scrim_att',\n",
       " 'avg_scrim_yds',\n",
       " 'avg_scrim_yds_per_att',\n",
       " 'avg_scrim_td',\n",
       " 'avg_tackles_solo',\n",
       " 'avg_tackles_assists',\n",
       " 'avg_tackles_total',\n",
       " 'avg_tackles_loss',\n",
       " 'avg_sacks',\n",
       " 'avg_def_int',\n",
       " 'avg_def_int_yds',\n",
       " 'avg_def_int_yds_per_int',\n",
       " 'avg_def_int_td',\n",
       " 'avg_pass_defended',\n",
       " 'avg_fumbles_rec',\n",
       " 'avg_fumbles_rec_yds',\n",
       " 'avg_fumbles_rec_td',\n",
       " 'avg_fumbles_forced',\n",
       " 'avg_punt_ret',\n",
       " 'avg_punt_ret_yds',\n",
       " 'avg_punt_ret_yds_per_ret',\n",
       " 'avg_punt_ret_td',\n",
       " 'avg_kick_ret',\n",
       " 'avg_kick_ret_yds',\n",
       " 'avg_kick_ret_yds_per_ret',\n",
       " 'avg_kick_ret_td']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuarterBacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 65)"
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qbs = players[players['position']=='QB'][avg_cols]\n",
    "\n",
    "qbs = pd.get_dummies(qbs, columns=['conference'], drop_first=True)\n",
    "qbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = qbs[qbs['draft_year']<=2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school', 'position', 'team_nfl'])\n",
    "y_train = qbs[qbs['draft_year']<=2015]['avg_grade']\n",
    "X_test = qbs[qbs['draft_year']>2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school', 'position', 'team_nfl'])\n",
    "y_test = qbs[qbs['draft_year']>2015]['avg_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3876497958917213"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "r2_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1604.2272431391093"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19091665773666455"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)\n",
    "rf_preds = rf_reg.predict(X_test)\n",
    "\n",
    "r2_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935.3610280285494"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "qb_model = Sequential()\n",
    "qb_model.add(Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'))\n",
    "qb_model.add(Dropout(0.5))\n",
    "qb_model.add(Dense(6, activation='relu'))\n",
    "# qb_model.add(Dense(5, activation='relu'))\n",
    "qb_model.add(Dense(1))\n",
    "\n",
    "adam = Adam(lr=0.01)b\n",
    "qb_model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples, validate on 36 samples\n",
      "Epoch 1/500\n",
      "63/63 [==============================] - 4s 66ms/step - loss: 65543.2496 - val_loss: 16477.9851\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 24743.7870 - val_loss: 5787.6897\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 9646.9174 - val_loss: 983.7105\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 11505.5669 - val_loss: 1773.7349\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 7384.8526 - val_loss: 1594.0509\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 5252.5938 - val_loss: 1845.5547\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 2733.4408 - val_loss: 1934.2743\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 1596.4282 - val_loss: 1933.9240\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 2020.9673 - val_loss: 1933.4684\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 2277.8386 - val_loss: 1932.9198\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 2228.6357 - val_loss: 1932.3090\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 2156.4984 - val_loss: 1931.6464\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 2120.6039 - val_loss: 1930.9042\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 1893.2236 - val_loss: 1930.1245\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 2097.4015 - val_loss: 1929.3054\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 2024.5627 - val_loss: 1928.4541\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 2127.1029 - val_loss: 1927.5772\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1896.9818 - val_loss: 1926.6748\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 0s 276us/step - loss: 2089.6400 - val_loss: 1925.7641\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 0s 276us/step - loss: 1829.6142 - val_loss: 1924.8490\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 2224.1836 - val_loss: 1923.9516\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1989.4698 - val_loss: 1923.0870\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1966.7907 - val_loss: 1922.2260\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1884.4203 - val_loss: 1921.3605\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 2066.8944 - val_loss: 1887.0572\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1861.6005 - val_loss: 1672.2866\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 1511.8396 - val_loss: 1340.3021\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1612.1194 - val_loss: 1152.2466\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1633.7557 - val_loss: 1097.5856\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1522.0080 - val_loss: 1099.7376\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1518.8517 - val_loss: 1119.5797\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1455.9516 - val_loss: 1156.8791\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1737.9745 - val_loss: 1165.1117\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1210.6267 - val_loss: 1147.2663\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1317.6066 - val_loss: 1084.0746\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 1201.9795 - val_loss: 1047.2359\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1163.5933 - val_loss: 1065.3202\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1346.9934 - val_loss: 1086.2487\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1267.0086 - val_loss: 1056.7361\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 937.4988 - val_loss: 1056.6109\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1192.8952 - val_loss: 1073.9734\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1146.4114 - val_loss: 1073.4002\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1107.2117 - val_loss: 1058.2273\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 976.8235 - val_loss: 1069.7872\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1128.6172 - val_loss: 1103.1958\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1052.9970 - val_loss: 1097.0011\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1050.2045 - val_loss: 1056.6518\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1084.2834 - val_loss: 1072.1454\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1178.7083 - val_loss: 1093.3457\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1161.4076 - val_loss: 1068.3260\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 893.8167 - val_loss: 1060.6124\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 909.9726 - val_loss: 1115.1754\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1204.3827 - val_loss: 1084.5114\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 904.9686 - val_loss: 1060.8008\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 1060.7806 - val_loss: 1048.8827\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 1172.1976 - val_loss: 1052.6518\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 979.0957 - val_loss: 1051.6427\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1017.8101 - val_loss: 1048.4341\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 962.3328 - val_loss: 1042.2312\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1002.2339 - val_loss: 1042.3558\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 944.4350 - val_loss: 1046.4107\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 913.8884 - val_loss: 1053.0925\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 0s 207us/step - loss: 951.6918 - val_loss: 1051.2852\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 963.5451 - val_loss: 1042.5169\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1166.0323 - val_loss: 1038.1644\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 837.4063 - val_loss: 1037.7315\n",
      "Epoch 67/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1032.4865 - val_loss: 1032.8126\n",
      "Epoch 68/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 1111.8833 - val_loss: 1034.8462\n",
      "Epoch 69/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1021.0303 - val_loss: 1039.4649\n",
      "Epoch 70/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 903.2785 - val_loss: 1033.6426\n",
      "Epoch 71/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1161.1826 - val_loss: 1032.4739\n",
      "Epoch 72/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1066.0490 - val_loss: 1033.4950\n",
      "Epoch 73/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 1070.9140 - val_loss: 1036.2405\n",
      "Epoch 74/500\n",
      "63/63 [==============================] - 0s 204us/step - loss: 943.8435 - val_loss: 1034.9090\n",
      "Epoch 75/500\n",
      "63/63 [==============================] - 0s 196us/step - loss: 990.9164 - val_loss: 1032.6982\n",
      "Epoch 76/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 994.9536 - val_loss: 1027.5768\n",
      "Epoch 77/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 1107.9281 - val_loss: 1025.3267\n",
      "Epoch 78/500\n",
      "63/63 [==============================] - 0s 212us/step - loss: 873.4926 - val_loss: 1024.1256\n",
      "Epoch 79/500\n",
      "63/63 [==============================] - 0s 212us/step - loss: 952.9296 - val_loss: 1026.4807\n",
      "Epoch 80/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1144.9734 - val_loss: 1027.7898\n",
      "Epoch 81/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 1089.5237 - val_loss: 1022.2713\n",
      "Epoch 82/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 999.9782 - val_loss: 1019.7577\n",
      "Epoch 83/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1060.1080 - val_loss: 1017.5903\n",
      "Epoch 84/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 1038.5099 - val_loss: 1015.4586\n",
      "Epoch 85/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 954.7249 - val_loss: 1012.6119\n",
      "Epoch 86/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1012.1516 - val_loss: 1010.6776\n",
      "Epoch 87/500\n",
      "63/63 [==============================] - 0s 212us/step - loss: 1094.5587 - val_loss: 1007.1481\n",
      "Epoch 88/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1144.5513 - val_loss: 1004.7105\n",
      "Epoch 89/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 925.0527 - val_loss: 1002.2685\n",
      "Epoch 90/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1034.9669 - val_loss: 1000.2980\n",
      "Epoch 91/500\n",
      "63/63 [==============================] - 0s 260us/step - loss: 882.4706 - val_loss: 999.7219\n",
      "Epoch 92/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 979.8696 - val_loss: 1001.7264\n",
      "Epoch 93/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 1012.9142 - val_loss: 999.4431\n",
      "Epoch 94/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 926.5121 - val_loss: 997.2937\n",
      "Epoch 95/500\n",
      "63/63 [==============================] - 0s 268us/step - loss: 929.7243 - val_loss: 992.9772\n",
      "Epoch 96/500\n",
      "63/63 [==============================] - 0s 260us/step - loss: 866.7600 - val_loss: 991.4975\n",
      "Epoch 97/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 895.7279 - val_loss: 986.5044\n",
      "Epoch 98/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 1031.3430 - val_loss: 982.0655\n",
      "Epoch 99/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 924.8186 - val_loss: 979.5511\n",
      "Epoch 100/500\n",
      "63/63 [==============================] - 0s 182us/step - loss: 926.1853 - val_loss: 975.1394\n",
      "Epoch 101/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1067.5127 - val_loss: 972.4376\n",
      "Epoch 102/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 940.5553 - val_loss: 974.4537\n",
      "Epoch 103/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 861.9560 - val_loss: 973.1647\n",
      "Epoch 104/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1027.0743 - val_loss: 966.7789\n",
      "Epoch 105/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1041.0258 - val_loss: 960.4217\n",
      "Epoch 106/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 976.4350 - val_loss: 959.9408\n",
      "Epoch 107/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 946.4008 - val_loss: 959.0952\n",
      "Epoch 108/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1039.3003 - val_loss: 956.9792\n",
      "Epoch 109/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1033.6699 - val_loss: 948.6029\n",
      "Epoch 110/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 989.4763 - val_loss: 949.7946\n",
      "Epoch 111/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 836.8707 - val_loss: 959.3129\n",
      "Epoch 112/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 775.2825 - val_loss: 957.7667\n",
      "Epoch 113/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 737.4692 - val_loss: 950.1409\n",
      "Epoch 114/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1161.4289 - val_loss: 933.8031\n",
      "Epoch 115/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 866.9409 - val_loss: 932.6394\n",
      "Epoch 116/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1032.7704 - val_loss: 940.6191\n",
      "Epoch 117/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 761.1116 - val_loss: 936.2792\n",
      "Epoch 118/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 915.8456 - val_loss: 919.9548\n",
      "Epoch 119/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 795.9552 - val_loss: 912.0541\n",
      "Epoch 120/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1084.4560 - val_loss: 907.8429\n",
      "Epoch 121/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 958.3641 - val_loss: 903.3136\n",
      "Epoch 122/500\n",
      "63/63 [==============================] - 0s 182us/step - loss: 695.4510 - val_loss: 900.8209\n",
      "Epoch 123/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 866.5964 - val_loss: 893.0677\n",
      "Epoch 124/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 878.5239 - val_loss: 888.1222\n",
      "Epoch 125/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 806.3421 - val_loss: 884.7644\n",
      "Epoch 126/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 856.2321 - val_loss: 883.1905\n",
      "Epoch 127/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 692.5259 - val_loss: 878.9499\n",
      "Epoch 128/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 845.8975 - val_loss: 875.9689\n",
      "Epoch 129/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 906.7999 - val_loss: 878.1646\n",
      "Epoch 130/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 856.1870 - val_loss: 878.1318\n",
      "Epoch 131/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 955.5590 - val_loss: 878.1731\n",
      "Epoch 132/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 861.4656 - val_loss: 885.1231\n",
      "Epoch 133/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 854.1566 - val_loss: 890.1632\n",
      "Epoch 134/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 831.3767 - val_loss: 884.7758\n",
      "Epoch 135/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 842.1247 - val_loss: 882.3249\n",
      "Epoch 136/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 796.3843 - val_loss: 888.7709\n",
      "Epoch 137/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 905.7198 - val_loss: 890.1091\n",
      "Epoch 138/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 738.5195 - val_loss: 887.5316\n",
      "Epoch 139/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 707.1558 - val_loss: 883.6060\n",
      "Epoch 140/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 686.8375 - val_loss: 879.7045\n",
      "Epoch 141/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 783.4371 - val_loss: 880.3119\n",
      "Epoch 142/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 825.1366 - val_loss: 882.1686\n",
      "Epoch 143/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 827.9423 - val_loss: 876.0515\n",
      "Epoch 144/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 853.7000 - val_loss: 865.1643\n",
      "Epoch 145/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 734.5613 - val_loss: 858.7260\n",
      "Epoch 146/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 797.9038 - val_loss: 849.4524\n",
      "Epoch 147/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 810.2064 - val_loss: 834.4353\n",
      "Epoch 148/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 815.2719 - val_loss: 828.4468\n",
      "Epoch 149/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 782.7101 - val_loss: 828.7342\n",
      "Epoch 150/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 872.2415 - val_loss: 817.2616\n",
      "Epoch 151/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 801.6290 - val_loss: 806.8781\n",
      "Epoch 152/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 197us/step - loss: 779.6493 - val_loss: 806.8996\n",
      "Epoch 153/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 790.8852 - val_loss: 817.0048\n",
      "Epoch 154/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 705.4079 - val_loss: 822.3255\n",
      "Epoch 155/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 767.2013 - val_loss: 810.4827\n",
      "Epoch 156/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 679.1873 - val_loss: 803.0630\n",
      "Epoch 157/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 847.6066 - val_loss: 806.5122\n",
      "Epoch 158/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 823.7739 - val_loss: 831.6191\n",
      "Epoch 159/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 701.8368 - val_loss: 813.3791\n",
      "Epoch 160/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 867.6729 - val_loss: 797.0053\n",
      "Epoch 161/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 714.6925 - val_loss: 790.9752\n",
      "Epoch 162/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 767.1014 - val_loss: 790.1612\n",
      "Epoch 163/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 706.5026 - val_loss: 785.8338\n",
      "Epoch 164/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 708.6767 - val_loss: 780.9645\n",
      "Epoch 165/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 681.0652 - val_loss: 777.1153\n",
      "Epoch 166/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 883.3745 - val_loss: 776.5131\n",
      "Epoch 167/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 752.5904 - val_loss: 776.8851\n",
      "Epoch 168/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 836.8499 - val_loss: 777.3342\n",
      "Epoch 169/500\n",
      "63/63 [==============================] - 0s 276us/step - loss: 778.5705 - val_loss: 782.5569\n",
      "Epoch 170/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 730.2695 - val_loss: 790.8162\n",
      "Epoch 171/500\n",
      "63/63 [==============================] - 0s 268us/step - loss: 602.5283 - val_loss: 797.1117\n",
      "Epoch 172/500\n",
      "63/63 [==============================] - 0s 252us/step - loss: 613.8989 - val_loss: 791.0735\n",
      "Epoch 173/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 639.1322 - val_loss: 785.0148\n",
      "Epoch 174/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 735.1392 - val_loss: 782.0022\n",
      "Epoch 175/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 708.8331 - val_loss: 781.1065\n",
      "Epoch 176/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 751.3185 - val_loss: 785.8922\n",
      "Epoch 177/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 552.6472 - val_loss: 786.3440\n",
      "Epoch 178/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 698.1491 - val_loss: 789.0352\n",
      "Epoch 179/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 780.9142 - val_loss: 791.4130\n",
      "Epoch 180/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 640.6172 - val_loss: 797.9070\n",
      "Epoch 181/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 784.9551 - val_loss: 793.6856\n",
      "Epoch 182/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 623.0261 - val_loss: 786.0947\n",
      "Epoch 183/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 735.8584 - val_loss: 778.2617\n",
      "Epoch 184/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 832.2290 - val_loss: 772.1832\n",
      "Epoch 185/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 881.1219 - val_loss: 772.7747\n",
      "Epoch 186/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 863.3094 - val_loss: 781.4723\n",
      "Epoch 187/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 709.4975 - val_loss: 778.3304\n",
      "Epoch 188/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 705.8805 - val_loss: 762.1767\n",
      "Epoch 189/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 708.8148 - val_loss: 776.7214\n",
      "Epoch 190/500\n",
      "63/63 [==============================] - 0s 237us/step - loss: 721.5805 - val_loss: 795.9698\n",
      "Epoch 191/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 670.0219 - val_loss: 788.9875\n",
      "Epoch 192/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 854.3540 - val_loss: 779.2528\n",
      "Epoch 193/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 622.6953 - val_loss: 777.9762\n",
      "Epoch 194/500\n",
      "63/63 [==============================] - 0s 212us/step - loss: 721.3313 - val_loss: 796.9317\n",
      "Epoch 195/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 768.3765 - val_loss: 791.1072\n",
      "Epoch 196/500\n",
      "63/63 [==============================] - 0s 229us/step - loss: 701.0551 - val_loss: 783.6933\n",
      "Epoch 197/500\n",
      "63/63 [==============================] - 0s 237us/step - loss: 772.4538 - val_loss: 799.1852\n",
      "Epoch 198/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 676.2281 - val_loss: 821.0935\n",
      "Epoch 199/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 683.6178 - val_loss: 806.4203\n",
      "Epoch 200/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 755.3807 - val_loss: 789.3248\n",
      "Epoch 201/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 808.0612 - val_loss: 824.0877\n",
      "Epoch 202/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 687.9175 - val_loss: 834.5561\n",
      "Epoch 203/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 725.8907 - val_loss: 814.8919\n",
      "Epoch 204/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 703.3621 - val_loss: 814.6228\n",
      "Epoch 205/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 620.4448 - val_loss: 845.9650\n",
      "Epoch 206/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 775.0128 - val_loss: 868.7079\n",
      "Epoch 207/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 737.2531 - val_loss: 833.8785\n",
      "Epoch 208/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 546.3092 - val_loss: 789.2038\n",
      "Epoch 209/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 767.1429 - val_loss: 768.6442\n",
      "Epoch 210/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 783.6897 - val_loss: 787.5835\n",
      "Epoch 211/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 655.7591 - val_loss: 781.2813\n",
      "Epoch 212/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 554.1920 - val_loss: 760.1484\n",
      "Epoch 213/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 649.2241 - val_loss: 763.5061\n",
      "Epoch 214/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 733.5308 - val_loss: 798.4877\n",
      "Epoch 215/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 788.9362 - val_loss: 795.1722\n",
      "Epoch 216/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 720.3016 - val_loss: 783.2103\n",
      "Epoch 217/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 683.5232 - val_loss: 768.3473\n",
      "Epoch 218/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 647.8429 - val_loss: 770.9114\n",
      "Epoch 219/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 601.8521 - val_loss: 769.8706\n",
      "Epoch 220/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 652.1887 - val_loss: 766.3123\n",
      "Epoch 221/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 617.5607 - val_loss: 771.5522\n",
      "Epoch 222/500\n",
      "63/63 [==============================] - 0s 284us/step - loss: 672.8465 - val_loss: 786.1761\n",
      "Epoch 223/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 835.0430 - val_loss: 789.1715\n",
      "Epoch 224/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 559.5530 - val_loss: 774.3463\n",
      "Epoch 225/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 635.3500 - val_loss: 765.5509\n",
      "Epoch 226/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 686.9158 - val_loss: 761.5426\n",
      "Epoch 227/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 624.9758 - val_loss: 763.4893\n",
      "Epoch 228/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 754.0159 - val_loss: 759.3875\n",
      "Epoch 229/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 581.4355 - val_loss: 755.4785\n",
      "Epoch 230/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 744.9667 - val_loss: 773.1116\n",
      "Epoch 231/500\n",
      "63/63 [==============================] - 0s 229us/step - loss: 639.9565 - val_loss: 798.7738\n",
      "Epoch 232/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 751.9490 - val_loss: 778.9089\n",
      "Epoch 233/500\n",
      "63/63 [==============================] - 0s 252us/step - loss: 801.9440 - val_loss: 749.9454\n",
      "Epoch 234/500\n",
      "63/63 [==============================] - 0s 260us/step - loss: 659.6398 - val_loss: 743.9343\n",
      "Epoch 235/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 735.1083 - val_loss: 762.5618\n",
      "Epoch 236/500\n",
      "63/63 [==============================] - 0s 182us/step - loss: 735.0932 - val_loss: 765.3013\n",
      "Epoch 237/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 917.0230 - val_loss: 749.9628\n",
      "Epoch 238/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 806.8111 - val_loss: 751.7392\n",
      "Epoch 239/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 664.3379 - val_loss: 767.1677\n",
      "Epoch 240/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 565.6416 - val_loss: 781.3924\n",
      "Epoch 241/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 735.3898 - val_loss: 782.4678\n",
      "Epoch 242/500\n",
      "63/63 [==============================] - 0s 237us/step - loss: 680.8498 - val_loss: 765.6952\n",
      "Epoch 243/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 804.6455 - val_loss: 766.1933\n",
      "Epoch 244/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 687.0482 - val_loss: 768.2183\n",
      "Epoch 245/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 672.4700 - val_loss: 762.8956\n",
      "Epoch 246/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 662.6181 - val_loss: 758.9191\n",
      "Epoch 247/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 615.5165 - val_loss: 767.3889\n",
      "Epoch 248/500\n",
      "63/63 [==============================] - 0s 252us/step - loss: 666.1806 - val_loss: 773.8849\n",
      "Epoch 249/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 702.4777 - val_loss: 759.2941\n",
      "Epoch 250/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 671.2943 - val_loss: 743.5077\n",
      "Epoch 251/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 687.2548 - val_loss: 740.9669\n",
      "Epoch 252/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 593.3628 - val_loss: 734.9291\n",
      "Epoch 253/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 624.1567 - val_loss: 724.3020\n",
      "Epoch 254/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 515.8728 - val_loss: 733.7095\n",
      "Epoch 255/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 634.6709 - val_loss: 786.6205\n",
      "Epoch 256/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 656.2215 - val_loss: 808.6554\n",
      "Epoch 257/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 592.4278 - val_loss: 789.9228\n",
      "Epoch 258/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 750.3550 - val_loss: 745.0482\n",
      "Epoch 259/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 726.5209 - val_loss: 749.9833\n",
      "Epoch 260/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 741.2404 - val_loss: 769.5233\n",
      "Epoch 261/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 673.6201 - val_loss: 746.9933\n",
      "Epoch 262/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 749.3104 - val_loss: 743.5274\n",
      "Epoch 263/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 759.0722 - val_loss: 744.9284\n",
      "Epoch 264/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 630.3776 - val_loss: 737.6986\n",
      "Epoch 265/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 577.5382 - val_loss: 737.9560\n",
      "Epoch 266/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 523.7242 - val_loss: 727.9930\n",
      "Epoch 267/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 616.7999 - val_loss: 721.4234\n",
      "Epoch 268/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 580.3531 - val_loss: 717.5882\n",
      "Epoch 269/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 797.0169 - val_loss: 722.5204\n",
      "Epoch 270/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 708.4678 - val_loss: 732.9124\n",
      "Epoch 271/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 655.3197 - val_loss: 751.3271\n",
      "Epoch 272/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 627.8902 - val_loss: 742.1978\n",
      "Epoch 273/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 699.1761 - val_loss: 733.2083\n",
      "Epoch 274/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 529.7921 - val_loss: 726.6001\n",
      "Epoch 275/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 639.2706 - val_loss: 726.5442\n",
      "Epoch 276/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 665.0646 - val_loss: 722.3553\n",
      "Epoch 277/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 563.3111 - val_loss: 734.6269\n",
      "Epoch 278/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 627.4240 - val_loss: 745.7866\n",
      "Epoch 279/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 543.1491 - val_loss: 743.4754\n",
      "Epoch 280/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 562.6475 - val_loss: 731.2594\n",
      "Epoch 281/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 684.4853 - val_loss: 710.0147\n",
      "Epoch 282/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 733.0019 - val_loss: 727.0437\n",
      "Epoch 283/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 632.6854 - val_loss: 752.0232\n",
      "Epoch 284/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 601.7291 - val_loss: 746.9187\n",
      "Epoch 285/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 702.4063 - val_loss: 739.2690\n",
      "Epoch 286/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 690.2647 - val_loss: 768.5185\n",
      "Epoch 287/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 564.4115 - val_loss: 768.5634\n",
      "Epoch 288/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 581.5972 - val_loss: 741.5828\n",
      "Epoch 289/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 740.5928 - val_loss: 725.2876\n",
      "Epoch 290/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 622.8982 - val_loss: 716.7465\n",
      "Epoch 291/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 577.1741 - val_loss: 714.3253\n",
      "Epoch 292/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 598.9011 - val_loss: 716.1644\n",
      "Epoch 293/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 608.6556 - val_loss: 708.2941\n",
      "Epoch 294/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 661.4122 - val_loss: 699.5716\n",
      "Epoch 295/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 642.1065 - val_loss: 703.7274\n",
      "Epoch 296/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 570.9822 - val_loss: 720.6728\n",
      "Epoch 297/500\n",
      "63/63 [==============================] - 0s 190us/step - loss: 498.0190 - val_loss: 733.0076\n",
      "Epoch 298/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 542.9670 - val_loss: 730.9674\n",
      "Epoch 299/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 567.4792 - val_loss: 718.4268\n",
      "Epoch 300/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 642.0654 - val_loss: 709.1907\n",
      "Epoch 301/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 691.9134 - val_loss: 697.4917\n",
      "Epoch 302/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 573.4457 - val_loss: 712.5122\n",
      "Epoch 303/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 798.9543 - val_loss: 720.6604\n",
      "Epoch 304/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 197us/step - loss: 767.2256 - val_loss: 725.1173\n",
      "Epoch 305/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 686.6838 - val_loss: 745.0741\n",
      "Epoch 306/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 624.0644 - val_loss: 780.7968\n",
      "Epoch 307/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 691.5699 - val_loss: 799.2046\n",
      "Epoch 308/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 672.5629 - val_loss: 781.2136\n",
      "Epoch 309/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 558.4689 - val_loss: 735.0147\n",
      "Epoch 310/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 529.1344 - val_loss: 712.1289\n",
      "Epoch 311/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 585.6041 - val_loss: 710.7413\n",
      "Epoch 312/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 610.0050 - val_loss: 732.3829\n",
      "Epoch 313/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 618.3368 - val_loss: 742.8581\n",
      "Epoch 314/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 593.6796 - val_loss: 716.6907\n",
      "Epoch 315/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 624.8919 - val_loss: 699.1413\n",
      "Epoch 316/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 709.0608 - val_loss: 704.6094\n",
      "Epoch 317/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 564.1097 - val_loss: 718.4419\n",
      "Epoch 318/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 734.0311 - val_loss: 714.8746\n",
      "Epoch 319/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 625.6238 - val_loss: 713.2996\n",
      "Epoch 320/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 624.9344 - val_loss: 715.9726\n",
      "Epoch 321/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 597.8059 - val_loss: 717.6497\n",
      "Epoch 322/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 675.4495 - val_loss: 725.1057\n",
      "Epoch 323/500\n",
      "63/63 [==============================] - 0s 236us/step - loss: 613.1138 - val_loss: 752.0050\n",
      "Epoch 324/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 510.1469 - val_loss: 758.7511\n",
      "Epoch 325/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 699.0004 - val_loss: 756.7790\n",
      "Epoch 326/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 535.0140 - val_loss: 732.6089\n",
      "Epoch 327/500\n",
      "63/63 [==============================] - 0s 260us/step - loss: 544.0343 - val_loss: 712.7902\n",
      "Epoch 328/500\n",
      "63/63 [==============================] - 0s 276us/step - loss: 505.3091 - val_loss: 701.7037\n",
      "Epoch 329/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 583.9175 - val_loss: 695.6825\n",
      "Epoch 330/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 573.3573 - val_loss: 695.7211\n",
      "Epoch 331/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 593.3369 - val_loss: 688.3531\n",
      "Epoch 332/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 640.1453 - val_loss: 686.8025\n",
      "Epoch 333/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 628.0542 - val_loss: 692.4089\n",
      "Epoch 334/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 590.6031 - val_loss: 700.3290\n",
      "Epoch 335/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 503.2483 - val_loss: 713.0877\n",
      "Epoch 336/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 696.4205 - val_loss: 710.2929\n",
      "Epoch 337/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 615.0832 - val_loss: 709.2817\n",
      "Epoch 338/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 679.4876 - val_loss: 713.2057\n",
      "Epoch 339/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 572.1404 - val_loss: 711.0672\n",
      "Epoch 340/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 703.9782 - val_loss: 712.1547\n",
      "Epoch 341/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 574.7227 - val_loss: 714.4185\n",
      "Epoch 342/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 584.4078 - val_loss: 721.5439\n",
      "Epoch 343/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 687.4325 - val_loss: 724.9451\n",
      "Epoch 344/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 556.2313 - val_loss: 730.6051\n",
      "Epoch 345/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 537.4317 - val_loss: 741.2516\n",
      "Epoch 346/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 514.7529 - val_loss: 752.7775\n",
      "Epoch 347/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 523.9768 - val_loss: 745.1133\n",
      "Epoch 348/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 651.5359 - val_loss: 728.9557\n",
      "Epoch 349/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 527.3970 - val_loss: 722.7416\n",
      "Epoch 350/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 709.8523 - val_loss: 706.5184\n",
      "Epoch 351/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 523.9294 - val_loss: 693.4932\n",
      "Epoch 352/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 639.1112 - val_loss: 659.2529\n",
      "Epoch 353/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 661.3696 - val_loss: 673.0749\n",
      "Epoch 354/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 639.3324 - val_loss: 671.2021\n",
      "Epoch 355/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 705.3592 - val_loss: 666.8161\n",
      "Epoch 356/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 592.4351 - val_loss: 702.8194\n",
      "Epoch 357/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 778.1040 - val_loss: 741.5241\n",
      "Epoch 358/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 612.8237 - val_loss: 749.7462\n",
      "Epoch 359/500\n",
      "63/63 [==============================] - 0s 229us/step - loss: 626.1756 - val_loss: 726.8666\n",
      "Epoch 360/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 640.1057 - val_loss: 723.1310\n",
      "Epoch 361/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 586.9534 - val_loss: 713.7807\n",
      "Epoch 362/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 553.2017 - val_loss: 709.4000\n",
      "Epoch 363/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 621.9772 - val_loss: 722.6863\n",
      "Epoch 364/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 575.4813 - val_loss: 730.1865\n",
      "Epoch 365/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 580.2420 - val_loss: 734.8084\n",
      "Epoch 366/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 582.4862 - val_loss: 735.1221\n",
      "Epoch 367/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 665.8226 - val_loss: 736.1529\n",
      "Epoch 368/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 624.6262 - val_loss: 739.3030\n",
      "Epoch 369/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 620.1314 - val_loss: 750.5957\n",
      "Epoch 370/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 645.3985 - val_loss: 751.6317\n",
      "Epoch 371/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 616.2965 - val_loss: 740.2937\n",
      "Epoch 372/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 639.8985 - val_loss: 733.0796\n",
      "Epoch 373/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 540.6545 - val_loss: 722.3438\n",
      "Epoch 374/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 638.8733 - val_loss: 712.2818\n",
      "Epoch 375/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 627.5879 - val_loss: 715.4825\n",
      "Epoch 376/500\n",
      "63/63 [==============================] - 0s 174us/step - loss: 534.9698 - val_loss: 712.9749\n",
      "Epoch 377/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 573.1045 - val_loss: 702.4455\n",
      "Epoch 378/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 465.2842 - val_loss: 696.2306\n",
      "Epoch 379/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 585.5414 - val_loss: 694.0283\n",
      "Epoch 380/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 498.5670 - val_loss: 695.9618\n",
      "Epoch 381/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 620.1838 - val_loss: 693.6417\n",
      "Epoch 382/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 635.1475 - val_loss: 690.2376\n",
      "Epoch 383/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 552.6154 - val_loss: 687.2852\n",
      "Epoch 384/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 527.0573 - val_loss: 683.5960\n",
      "Epoch 385/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 512.2471 - val_loss: 684.7929\n",
      "Epoch 386/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 584.7078 - val_loss: 686.6036\n",
      "Epoch 387/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 625.7016 - val_loss: 689.0914\n",
      "Epoch 388/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 614.1985 - val_loss: 695.7648\n",
      "Epoch 389/500\n",
      "63/63 [==============================] - 0s 252us/step - loss: 619.5457 - val_loss: 697.6491\n",
      "Epoch 390/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 547.2292 - val_loss: 707.0955\n",
      "Epoch 391/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 685.7406 - val_loss: 710.1149\n",
      "Epoch 392/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 559.7017 - val_loss: 704.7589\n",
      "Epoch 393/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 568.9044 - val_loss: 697.4860\n",
      "Epoch 394/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 505.8691 - val_loss: 698.6137\n",
      "Epoch 395/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 640.5620 - val_loss: 696.1664\n",
      "Epoch 396/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 592.0591 - val_loss: 704.7327\n",
      "Epoch 397/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 735.5086 - val_loss: 715.1898\n",
      "Epoch 398/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 657.6980 - val_loss: 704.5563\n",
      "Epoch 399/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 530.5977 - val_loss: 703.9816\n",
      "Epoch 400/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 735.3834 - val_loss: 707.6732\n",
      "Epoch 401/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 554.9063 - val_loss: 708.7114\n",
      "Epoch 402/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 563.7708 - val_loss: 702.6052\n",
      "Epoch 403/500\n",
      "63/63 [==============================] - 0s 260us/step - loss: 603.4588 - val_loss: 703.7251\n",
      "Epoch 404/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 475.4550 - val_loss: 702.3766\n",
      "Epoch 405/500\n",
      "63/63 [==============================] - 0s 244us/step - loss: 532.8165 - val_loss: 713.3866\n",
      "Epoch 406/500\n",
      "63/63 [==============================] - 0s 291us/step - loss: 526.7594 - val_loss: 722.7842\n",
      "Epoch 407/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 629.1967 - val_loss: 721.6505\n",
      "Epoch 408/500\n",
      "63/63 [==============================] - 0s 165us/step - loss: 589.3747 - val_loss: 705.8038\n",
      "Epoch 409/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 441.7890 - val_loss: 693.9296\n",
      "Epoch 410/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 514.3010 - val_loss: 689.3680\n",
      "Epoch 411/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 669.0794 - val_loss: 692.9018\n",
      "Epoch 412/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 545.7995 - val_loss: 688.3961\n",
      "Epoch 413/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 587.4038 - val_loss: 682.3543\n",
      "Epoch 414/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 494.8732 - val_loss: 676.8505\n",
      "Epoch 415/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 491.5441 - val_loss: 675.6845\n",
      "Epoch 416/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 627.1311 - val_loss: 665.1285\n",
      "Epoch 417/500\n",
      "63/63 [==============================] - 0s 228us/step - loss: 532.1714 - val_loss: 656.6216\n",
      "Epoch 418/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 562.8887 - val_loss: 661.2249\n",
      "Epoch 419/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 597.0839 - val_loss: 667.9177\n",
      "Epoch 420/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 544.9639 - val_loss: 686.1948\n",
      "Epoch 421/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 558.4780 - val_loss: 701.2401\n",
      "Epoch 422/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 585.8062 - val_loss: 706.9061\n",
      "Epoch 423/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 512.4829 - val_loss: 705.5957\n",
      "Epoch 424/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 635.5493 - val_loss: 707.4417\n",
      "Epoch 425/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 554.6763 - val_loss: 732.3096\n",
      "Epoch 426/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 675.5479 - val_loss: 692.7819\n",
      "Epoch 427/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 570.5814 - val_loss: 677.5713\n",
      "Epoch 428/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 637.1670 - val_loss: 705.4477\n",
      "Epoch 429/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 657.4953 - val_loss: 705.5851\n",
      "Epoch 430/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 604.4580 - val_loss: 675.9578\n",
      "Epoch 431/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 543.0964 - val_loss: 685.6971\n",
      "Epoch 432/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 736.7904 - val_loss: 692.1171\n",
      "Epoch 433/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 736.5083 - val_loss: 685.0657\n",
      "Epoch 434/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 600.9650 - val_loss: 714.5647\n",
      "Epoch 435/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 626.6785 - val_loss: 737.2639\n",
      "Epoch 436/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 543.1947 - val_loss: 722.2429\n",
      "Epoch 437/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 658.7722 - val_loss: 718.8685\n",
      "Epoch 438/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 698.2353 - val_loss: 713.1295\n",
      "Epoch 439/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 535.6811 - val_loss: 702.4443\n",
      "Epoch 440/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 445.9652 - val_loss: 701.9237\n",
      "Epoch 441/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 581.6704 - val_loss: 681.0718\n",
      "Epoch 442/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 565.9474 - val_loss: 681.5111\n",
      "Epoch 443/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 486.1780 - val_loss: 686.6147\n",
      "Epoch 444/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 688.3875 - val_loss: 689.3771\n",
      "Epoch 445/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 531.8358 - val_loss: 722.3452\n",
      "Epoch 446/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 542.3766 - val_loss: 730.7304\n",
      "Epoch 447/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 658.1046 - val_loss: 697.7428\n",
      "Epoch 448/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 536.4072 - val_loss: 712.9333\n",
      "Epoch 449/500\n",
      "63/63 [==============================] - 0s 190us/step - loss: 660.1576 - val_loss: 737.4789\n",
      "Epoch 450/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 586.1383 - val_loss: 705.0838\n",
      "Epoch 451/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 520.4424 - val_loss: 692.7352\n",
      "Epoch 452/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 545.1574 - val_loss: 710.7148\n",
      "Epoch 453/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 611.9492 - val_loss: 695.2164\n",
      "Epoch 454/500\n",
      "63/63 [==============================] - 0s 252us/step - loss: 627.8067 - val_loss: 694.6354\n",
      "Epoch 455/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 612.2379 - val_loss: 706.7132\n",
      "Epoch 456/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 221us/step - loss: 673.0789 - val_loss: 697.0196\n",
      "Epoch 457/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 526.5674 - val_loss: 681.6867\n",
      "Epoch 458/500\n",
      "63/63 [==============================] - 0s 218us/step - loss: 665.2596 - val_loss: 700.4949\n",
      "Epoch 459/500\n",
      "63/63 [==============================] - 0s 174us/step - loss: 497.5626 - val_loss: 702.5873\n",
      "Epoch 460/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 499.9537 - val_loss: 690.5390\n",
      "Epoch 461/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 541.5196 - val_loss: 690.8341\n",
      "Epoch 462/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 514.8170 - val_loss: 692.1716\n",
      "Epoch 463/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 629.4392 - val_loss: 687.7514\n",
      "Epoch 464/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 542.9433 - val_loss: 685.1601\n",
      "Epoch 465/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 524.4474 - val_loss: 690.1565\n",
      "Epoch 466/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 538.4182 - val_loss: 685.3388\n",
      "Epoch 467/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 436.2727 - val_loss: 680.0361\n",
      "Epoch 468/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 536.5218 - val_loss: 680.5011\n",
      "Epoch 469/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 513.7265 - val_loss: 683.5880\n",
      "Epoch 470/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 607.7682 - val_loss: 684.8879\n",
      "Epoch 471/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 512.4802 - val_loss: 675.8056\n",
      "Epoch 472/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 582.5604 - val_loss: 682.9711\n",
      "Epoch 473/500\n",
      "63/63 [==============================] - 0s 190us/step - loss: 568.4252 - val_loss: 694.7754\n",
      "Epoch 474/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 471.9556 - val_loss: 697.5471\n",
      "Epoch 475/500\n",
      "63/63 [==============================] - 0s 174us/step - loss: 539.5695 - val_loss: 697.9836\n",
      "Epoch 476/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 537.0964 - val_loss: 710.4474\n",
      "Epoch 477/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 492.6607 - val_loss: 716.1394\n",
      "Epoch 478/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 541.5681 - val_loss: 695.9515\n",
      "Epoch 479/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 585.1257 - val_loss: 683.5049\n",
      "Epoch 480/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 568.4731 - val_loss: 693.6092\n",
      "Epoch 481/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 555.5939 - val_loss: 691.0283\n",
      "Epoch 482/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 506.2981 - val_loss: 682.6157\n",
      "Epoch 483/500\n",
      "63/63 [==============================] - 0s 221us/step - loss: 493.1063 - val_loss: 675.5716\n",
      "Epoch 484/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 611.5275 - val_loss: 668.4413\n",
      "Epoch 485/500\n",
      "63/63 [==============================] - 0s 268us/step - loss: 505.7165 - val_loss: 672.0940\n",
      "Epoch 486/500\n",
      "63/63 [==============================] - 0s 252us/step - loss: 555.6087 - val_loss: 669.8081\n",
      "Epoch 487/500\n",
      "63/63 [==============================] - 0s 220us/step - loss: 557.2662 - val_loss: 680.4696\n",
      "Epoch 488/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 538.6160 - val_loss: 682.5399\n",
      "Epoch 489/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 560.8869 - val_loss: 660.0471\n",
      "Epoch 490/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 425.3259 - val_loss: 655.4065\n",
      "Epoch 491/500\n",
      "63/63 [==============================] - 0s 181us/step - loss: 616.9241 - val_loss: 664.0002\n",
      "Epoch 492/500\n",
      "63/63 [==============================] - 0s 213us/step - loss: 533.4781 - val_loss: 663.0691\n",
      "Epoch 493/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 531.3864 - val_loss: 649.2745\n",
      "Epoch 494/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 571.7981 - val_loss: 654.2078\n",
      "Epoch 495/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 527.4415 - val_loss: 659.9243\n",
      "Epoch 496/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 531.5245 - val_loss: 668.8367\n",
      "Epoch 497/500\n",
      "63/63 [==============================] - 0s 189us/step - loss: 610.4842 - val_loss: 664.7934\n",
      "Epoch 498/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 553.1861 - val_loss: 683.4703\n",
      "Epoch 499/500\n",
      "63/63 [==============================] - 0s 205us/step - loss: 501.1865 - val_loss: 699.5447\n",
      "Epoch 500/500\n",
      "63/63 [==============================] - 0s 197us/step - loss: 539.9853 - val_loss: 676.5424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d61f5257b8>"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qb_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4147936958749582"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, qb_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676.5423802990983"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, qb_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 64)"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbs = players[(players['position']=='RB') | (players['position']=='FB')][avg_cols]\n",
    "\n",
    "rbs = pd.get_dummies(rbs, columns=['conference'], drop_first=True)\n",
    "rbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rbs[rbs['draft_year']<=2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school', 'position', 'team_nfl'])\n",
    "y_train = rbs[rbs['draft_year']<=2015]['avg_grade']\n",
    "X_test = rbs[rbs['draft_year']>2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school', 'position', 'team_nfl'])\n",
    "y_test = rbs[rbs['draft_year']>2015]['avg_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9128532026234442"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "r2_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1979.2197254360285"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07061365985902468"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)\n",
    "rf_preds = rf_reg.predict(X_test)\n",
    "\n",
    "r2_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961.6314385416666"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rb_model = Sequential()\n",
    "rb_model.add(Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'))\n",
    "rb_model.add(Dropout(0.5))\n",
    "# rb_model.add(Dense(12, activation='relu'))\n",
    "# rb_model.add(Dense(10, activation='relu'))\n",
    "rb_model.add(Dense(1))\n",
    "\n",
    "adam = Adam(lr=0.01)\n",
    "rb_model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples, validate on 36 samples\n",
      "Epoch 1/150\n",
      "63/63 [==============================] - 4s 65ms/step - loss: 464261.4231 - val_loss: 59283.9366\n",
      "Epoch 2/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 166924.4938 - val_loss: 46847.0514\n",
      "Epoch 3/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 141624.4876 - val_loss: 102181.7917\n",
      "Epoch 4/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 204299.7205 - val_loss: 73844.9323\n",
      "Epoch 5/150\n",
      "63/63 [==============================] - 0s 237us/step - loss: 139521.1656 - val_loss: 18703.6643\n",
      "Epoch 6/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 88013.4501 - val_loss: 2286.3856\n",
      "Epoch 7/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 46713.0326 - val_loss: 11856.1194\n",
      "Epoch 8/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 51243.4437 - val_loss: 19674.6634\n",
      "Epoch 9/150\n",
      "63/63 [==============================] - 0s 244us/step - loss: 46938.0453 - val_loss: 16481.7230\n",
      "Epoch 10/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 40515.8533 - val_loss: 8125.9154\n",
      "Epoch 11/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 37043.3030 - val_loss: 2694.3284\n",
      "Epoch 12/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 16862.4592 - val_loss: 1979.8033\n",
      "Epoch 13/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 13327.2031 - val_loss: 3336.7242\n",
      "Epoch 14/150\n",
      "63/63 [==============================] - 0s 165us/step - loss: 10467.0292 - val_loss: 4672.3135\n",
      "Epoch 15/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 16248.5769 - val_loss: 4787.9579\n",
      "Epoch 16/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 11896.2095 - val_loss: 3664.0188\n",
      "Epoch 17/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 9510.0333 - val_loss: 2547.4366\n",
      "Epoch 18/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 5706.2480 - val_loss: 1676.1645\n",
      "Epoch 19/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 4752.9087 - val_loss: 1193.6785\n",
      "Epoch 20/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 5421.6797 - val_loss: 1017.1337\n",
      "Epoch 21/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 5178.3755 - val_loss: 1043.8832\n",
      "Epoch 22/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 5486.1825 - val_loss: 1067.3331\n",
      "Epoch 23/150\n",
      "63/63 [==============================] - 0s 237us/step - loss: 2868.1314 - val_loss: 1045.7868\n",
      "Epoch 24/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 3497.3128 - val_loss: 1031.8268\n",
      "Epoch 25/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 2698.2640 - val_loss: 993.8274\n",
      "Epoch 26/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 2305.5466 - val_loss: 967.4232\n",
      "Epoch 27/150\n",
      "63/63 [==============================] - 0s 236us/step - loss: 2554.6830 - val_loss: 965.8414\n",
      "Epoch 28/150\n",
      "63/63 [==============================] - 0s 236us/step - loss: 2391.6332 - val_loss: 981.8006\n",
      "Epoch 29/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 2366.0154 - val_loss: 1007.0963\n",
      "Epoch 30/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 2211.1385 - val_loss: 1032.5500\n",
      "Epoch 31/150\n",
      "63/63 [==============================] - 0s 276us/step - loss: 2311.3348 - val_loss: 1046.7659\n",
      "Epoch 32/150\n",
      "63/63 [==============================] - 0s 260us/step - loss: 2050.3216 - val_loss: 1045.8222\n",
      "Epoch 33/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1832.6842 - val_loss: 1033.3320\n",
      "Epoch 34/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 2006.7641 - val_loss: 1011.7161\n",
      "Epoch 35/150\n",
      "63/63 [==============================] - 0s 244us/step - loss: 1558.0319 - val_loss: 991.9419\n",
      "Epoch 36/150\n",
      "63/63 [==============================] - 0s 244us/step - loss: 1604.9284 - val_loss: 973.7679\n",
      "Epoch 37/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1802.5004 - val_loss: 956.8330\n",
      "Epoch 38/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1625.9632 - val_loss: 938.6017\n",
      "Epoch 39/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1572.6046 - val_loss: 923.7033\n",
      "Epoch 40/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1353.4432 - val_loss: 911.0061\n",
      "Epoch 41/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1718.0109 - val_loss: 903.1285\n",
      "Epoch 42/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1081.8845 - val_loss: 896.5483\n",
      "Epoch 43/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1477.8353 - val_loss: 893.5721\n",
      "Epoch 44/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 1462.9718 - val_loss: 892.7314\n",
      "Epoch 45/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1132.5713 - val_loss: 890.1513\n",
      "Epoch 46/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1386.2752 - val_loss: 888.3158\n",
      "Epoch 47/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1258.4428 - val_loss: 888.3477\n",
      "Epoch 48/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1268.6999 - val_loss: 895.1495\n",
      "Epoch 49/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 998.7086 - val_loss: 903.7921\n",
      "Epoch 50/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1185.7683 - val_loss: 910.4825\n",
      "Epoch 51/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 1021.1652 - val_loss: 914.5358\n",
      "Epoch 52/150\n",
      "63/63 [==============================] - 0s 189us/step - loss: 1054.6777 - val_loss: 918.6273\n",
      "Epoch 53/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1165.2815 - val_loss: 920.8654\n",
      "Epoch 54/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 1163.5197 - val_loss: 920.2456\n",
      "Epoch 55/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1237.1348 - val_loss: 917.7387\n",
      "Epoch 56/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 790.4056 - val_loss: 913.1577\n",
      "Epoch 57/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 853.4133 - val_loss: 907.7475\n",
      "Epoch 58/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 973.2526 - val_loss: 903.5179\n",
      "Epoch 59/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1019.8449 - val_loss: 897.1905\n",
      "Epoch 60/150\n",
      "63/63 [==============================] - 0s 173us/step - loss: 1045.9928 - val_loss: 891.2055\n",
      "Epoch 61/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1031.5974 - val_loss: 888.2381\n",
      "Epoch 62/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 1089.5564 - val_loss: 888.2020\n",
      "Epoch 63/150\n",
      "63/63 [==============================] - 0s 181us/step - loss: 981.0343 - val_loss: 889.0942\n",
      "Epoch 64/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 834.9534 - val_loss: 890.8781\n",
      "Epoch 65/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 870.1148 - val_loss: 890.5334\n",
      "Epoch 66/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 902.5142 - val_loss: 890.5135\n",
      "Epoch 67/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 894.8398 - val_loss: 889.4272\n",
      "Epoch 68/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1081.3124 - val_loss: 890.4809\n",
      "Epoch 69/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 853.1373 - val_loss: 891.7200\n",
      "Epoch 70/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 1004.6733 - val_loss: 892.7925\n",
      "Epoch 71/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 864.3857 - val_loss: 894.3894\n",
      "Epoch 72/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 838.0512 - val_loss: 892.9843\n",
      "Epoch 73/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 852.3663 - val_loss: 891.3134\n",
      "Epoch 74/150\n",
      "63/63 [==============================] - 0s 236us/step - loss: 775.2140 - val_loss: 893.3790\n",
      "Epoch 75/150\n",
      "63/63 [==============================] - 0s 181us/step - loss: 960.1613 - val_loss: 896.4663\n",
      "Epoch 76/150\n",
      "63/63 [==============================] - 0s 173us/step - loss: 1120.5936 - val_loss: 896.0997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 945.8310 - val_loss: 893.9184\n",
      "Epoch 78/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 854.4407 - val_loss: 887.3808\n",
      "Epoch 79/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 939.5596 - val_loss: 877.4611\n",
      "Epoch 80/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 881.0972 - val_loss: 874.0325\n",
      "Epoch 81/150\n",
      "63/63 [==============================] - 0s 244us/step - loss: 674.1513 - val_loss: 871.7493\n",
      "Epoch 82/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 824.1918 - val_loss: 872.6878\n",
      "Epoch 83/150\n",
      "63/63 [==============================] - 0s 174us/step - loss: 996.8225 - val_loss: 873.2351\n",
      "Epoch 84/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1026.3719 - val_loss: 873.5593\n",
      "Epoch 85/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 1029.1537 - val_loss: 874.7678\n",
      "Epoch 86/150\n",
      "63/63 [==============================] - 0s 229us/step - loss: 715.6709 - val_loss: 873.4761\n",
      "Epoch 87/150\n",
      "63/63 [==============================] - 0s 189us/step - loss: 780.6440 - val_loss: 872.8846\n",
      "Epoch 88/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 1077.0257 - val_loss: 870.8126\n",
      "Epoch 89/150\n",
      "63/63 [==============================] - 0s 207us/step - loss: 791.9324 - val_loss: 868.1021\n",
      "Epoch 90/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 923.3758 - val_loss: 867.4613\n",
      "Epoch 91/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 849.4668 - val_loss: 868.5669\n",
      "Epoch 92/150\n",
      "63/63 [==============================] - 0s 204us/step - loss: 689.7025 - val_loss: 870.9129\n",
      "Epoch 93/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 888.6513 - val_loss: 870.8486\n",
      "Epoch 94/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 850.8436 - val_loss: 871.4514\n",
      "Epoch 95/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 802.5197 - val_loss: 871.9012\n",
      "Epoch 96/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 865.9608 - val_loss: 869.0444\n",
      "Epoch 97/150\n",
      "63/63 [==============================] - 0s 189us/step - loss: 745.2856 - val_loss: 860.6422\n",
      "Epoch 98/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 962.3889 - val_loss: 854.8499\n",
      "Epoch 99/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 1021.5738 - val_loss: 846.7234\n",
      "Epoch 100/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 841.8991 - val_loss: 839.2909\n",
      "Epoch 101/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 1039.1403 - val_loss: 833.7420\n",
      "Epoch 102/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 704.2774 - val_loss: 831.6459\n",
      "Epoch 103/150\n",
      "63/63 [==============================] - 0s 260us/step - loss: 880.0868 - val_loss: 830.7228\n",
      "Epoch 104/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 924.9550 - val_loss: 830.3654\n",
      "Epoch 105/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 713.9108 - val_loss: 829.8610\n",
      "Epoch 106/150\n",
      "63/63 [==============================] - 0s 204us/step - loss: 874.7419 - val_loss: 830.2240\n",
      "Epoch 107/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 859.8030 - val_loss: 827.1096\n",
      "Epoch 108/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 981.0404 - val_loss: 825.2870\n",
      "Epoch 109/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 781.9533 - val_loss: 826.0544\n",
      "Epoch 110/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 754.8354 - val_loss: 827.4044\n",
      "Epoch 111/150\n",
      "63/63 [==============================] - 0s 196us/step - loss: 903.3099 - val_loss: 829.8403\n",
      "Epoch 112/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 834.9292 - val_loss: 832.8419\n",
      "Epoch 113/150\n",
      "63/63 [==============================] - 0s 267us/step - loss: 755.7145 - val_loss: 833.4529\n",
      "Epoch 114/150\n",
      "63/63 [==============================] - 0s 189us/step - loss: 757.9095 - val_loss: 837.6090\n",
      "Epoch 115/150\n",
      "63/63 [==============================] - 0s 204us/step - loss: 920.8980 - val_loss: 837.0278\n",
      "Epoch 116/150\n",
      "63/63 [==============================] - 0s 212us/step - loss: 964.4161 - val_loss: 831.3143\n",
      "Epoch 117/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 965.5400 - val_loss: 827.5060\n",
      "Epoch 118/150\n",
      "63/63 [==============================] - 0s 204us/step - loss: 929.5609 - val_loss: 825.7546\n",
      "Epoch 119/150\n",
      "63/63 [==============================] - 0s 196us/step - loss: 796.1566 - val_loss: 823.8380\n",
      "Epoch 120/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 885.4072 - val_loss: 825.1510\n",
      "Epoch 121/150\n",
      "63/63 [==============================] - 0s 189us/step - loss: 798.6842 - val_loss: 825.4839\n",
      "Epoch 122/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 797.7045 - val_loss: 825.1357\n",
      "Epoch 123/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 907.5076 - val_loss: 823.4305\n",
      "Epoch 124/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 689.1096 - val_loss: 826.3930\n",
      "Epoch 125/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 666.3879 - val_loss: 831.6442\n",
      "Epoch 126/150\n",
      "63/63 [==============================] - 0s 196us/step - loss: 715.2615 - val_loss: 836.4520\n",
      "Epoch 127/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 758.8941 - val_loss: 838.6525\n",
      "Epoch 128/150\n",
      "63/63 [==============================] - 0s 204us/step - loss: 850.7756 - val_loss: 839.0838\n",
      "Epoch 129/150\n",
      "63/63 [==============================] - 0s 189us/step - loss: 927.4274 - val_loss: 833.8636\n",
      "Epoch 130/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 866.1481 - val_loss: 832.3005\n",
      "Epoch 131/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 784.7077 - val_loss: 830.7349\n",
      "Epoch 132/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 809.5711 - val_loss: 829.2244\n",
      "Epoch 133/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 812.3428 - val_loss: 827.9403\n",
      "Epoch 134/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 854.9991 - val_loss: 825.2994\n",
      "Epoch 135/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 917.1081 - val_loss: 824.0058\n",
      "Epoch 136/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 885.3158 - val_loss: 820.2918\n",
      "Epoch 137/150\n",
      "63/63 [==============================] - 0s 228us/step - loss: 821.1190 - val_loss: 818.9954\n",
      "Epoch 138/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 734.4238 - val_loss: 818.5508\n",
      "Epoch 139/150\n",
      "63/63 [==============================] - 0s 190us/step - loss: 941.6886 - val_loss: 819.2770\n",
      "Epoch 140/150\n",
      "63/63 [==============================] - 0s 197us/step - loss: 805.1843 - val_loss: 816.9413\n",
      "Epoch 141/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 863.4119 - val_loss: 808.8851\n",
      "Epoch 142/150\n",
      "63/63 [==============================] - 0s 221us/step - loss: 687.5857 - val_loss: 803.1264\n",
      "Epoch 143/150\n",
      "63/63 [==============================] - 0s 196us/step - loss: 932.0393 - val_loss: 796.4449\n",
      "Epoch 144/150\n",
      "63/63 [==============================] - 0s 212us/step - loss: 768.0227 - val_loss: 793.9696\n",
      "Epoch 145/150\n",
      "63/63 [==============================] - 0s 236us/step - loss: 562.8044 - val_loss: 793.5120\n",
      "Epoch 146/150\n",
      "63/63 [==============================] - 0s 220us/step - loss: 765.7458 - val_loss: 792.9230\n",
      "Epoch 147/150\n",
      "63/63 [==============================] - 0s 213us/step - loss: 917.7138 - val_loss: 796.5267\n",
      "Epoch 148/150\n",
      "63/63 [==============================] - 0s 204us/step - loss: 889.4281 - val_loss: 798.8131\n",
      "Epoch 149/150\n",
      "63/63 [==============================] - 0s 205us/step - loss: 691.2820 - val_loss: 800.5857\n",
      "Epoch 150/150\n",
      "63/63 [==============================] - 0s 212us/step - loss: 772.5000 - val_loss: 805.2209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d61fac5dd8>"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3034872665364723"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, rb_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805.2209610260114"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rb_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, 64)"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs = players[(players['position']=='WR') | (players['position']=='TE')][avg_cols]\n",
    "\n",
    "recs = pd.get_dummies(recs, columns=['conference'], drop_first=True)\n",
    "recs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = recs[recs['draft_year']<=2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school', 'position', 'team_nfl'])\n",
    "y_train = recs[recs['draft_year']<=2015]['avg_grade']\n",
    "X_test = recs[recs['draft_year']>2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school', 'position', 'team_nfl'])\n",
    "y_test = recs[recs['draft_year']>2015]['avg_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11299930471576525"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "r2_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724.399954580358"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2795281733081494"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)\n",
    "rf_preds = rf_reg.predict(X_test)\n",
    "\n",
    "r2_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588.3983646312261"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rec_model = Sequential()\n",
    "rec_model.add(Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'))\n",
    "rec_model.add(Dropout(0.5))\n",
    "# rec_model.add(Dense(12, activation='relu'))\n",
    "# rec_model.add(Dense(10, activation='relu'))\n",
    "rec_model.add(Dense(1))\n",
    "\n",
    "adam = Adam(lr=0.01)\n",
    "rec_model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 273 samples, validate on 116 samples\n",
      "Epoch 1/500\n",
      "273/273 [==============================] - 4s 15ms/step - loss: 42110.9236 - val_loss: 3383.0757\n",
      "Epoch 2/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 15963.8492 - val_loss: 2499.6549\n",
      "Epoch 3/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 8810.1800 - val_loss: 2933.4230\n",
      "Epoch 4/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 4134.8912 - val_loss: 1239.1694\n",
      "Epoch 5/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 2510.0992 - val_loss: 1101.4165\n",
      "Epoch 6/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 1685.4064 - val_loss: 1170.9424\n",
      "Epoch 7/500\n",
      "273/273 [==============================] - 0s 154us/step - loss: 1409.3794 - val_loss: 1058.8991\n",
      "Epoch 8/500\n",
      "273/273 [==============================] - 0s 155us/step - loss: 1378.4357 - val_loss: 1053.8507\n",
      "Epoch 9/500\n",
      "273/273 [==============================] - 0s 173us/step - loss: 1348.2807 - val_loss: 1085.5614\n",
      "Epoch 10/500\n",
      "273/273 [==============================] - 0s 154us/step - loss: 1254.7189 - val_loss: 1102.2176\n",
      "Epoch 11/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 1220.0085 - val_loss: 1064.9188\n",
      "Epoch 12/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 1332.8133 - val_loss: 1024.3989\n",
      "Epoch 13/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 1292.2391 - val_loss: 1012.7280\n",
      "Epoch 14/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 1132.1947 - val_loss: 1004.0916\n",
      "Epoch 15/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 1223.9793 - val_loss: 1010.8822\n",
      "Epoch 16/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 1179.4876 - val_loss: 993.0764\n",
      "Epoch 17/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 1288.9419 - val_loss: 977.8300\n",
      "Epoch 18/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 1344.9648 - val_loss: 975.6763\n",
      "Epoch 19/500\n",
      "273/273 [==============================] - 0s 126us/step - loss: 1207.3524 - val_loss: 978.2443\n",
      "Epoch 20/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 1319.1931 - val_loss: 966.6758\n",
      "Epoch 21/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 1126.2549 - val_loss: 960.9500\n",
      "Epoch 22/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 1120.4065 - val_loss: 979.5109\n",
      "Epoch 23/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 1185.0238 - val_loss: 953.1717\n",
      "Epoch 24/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 1094.9349 - val_loss: 967.2809\n",
      "Epoch 25/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 1094.5387 - val_loss: 989.6111\n",
      "Epoch 26/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 1190.9221 - val_loss: 926.2172\n",
      "Epoch 27/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 1248.8192 - val_loss: 922.3477\n",
      "Epoch 28/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 1144.7683 - val_loss: 932.1449\n",
      "Epoch 29/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 1113.7410 - val_loss: 940.7712\n",
      "Epoch 30/500\n",
      "273/273 [==============================] - 0s 126us/step - loss: 1098.9954 - val_loss: 952.2666\n",
      "Epoch 31/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 1164.2407 - val_loss: 932.9302\n",
      "Epoch 32/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 1072.2724 - val_loss: 928.3807\n",
      "Epoch 33/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 1049.9946 - val_loss: 966.2903\n",
      "Epoch 34/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 1125.3348 - val_loss: 956.5707\n",
      "Epoch 35/500\n",
      "273/273 [==============================] - 0s 118us/step - loss: 1085.1813 - val_loss: 931.5272\n",
      "Epoch 36/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 1110.9489 - val_loss: 952.3106\n",
      "Epoch 37/500\n",
      "273/273 [==============================] - 0s 169us/step - loss: 1169.1395 - val_loss: 914.0793\n",
      "Epoch 38/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 1076.8741 - val_loss: 964.7424\n",
      "Epoch 39/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 1030.0425 - val_loss: 901.0954\n",
      "Epoch 40/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 1087.0447 - val_loss: 903.0821\n",
      "Epoch 41/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 1118.6816 - val_loss: 925.3573\n",
      "Epoch 42/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 1030.8986 - val_loss: 891.8210\n",
      "Epoch 43/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 988.5857 - val_loss: 888.0375\n",
      "Epoch 44/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 1115.7635 - val_loss: 899.5035\n",
      "Epoch 45/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 1004.3656 - val_loss: 888.2629\n",
      "Epoch 46/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 1076.0364 - val_loss: 873.6842\n",
      "Epoch 47/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 1055.0734 - val_loss: 889.1063\n",
      "Epoch 48/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 1039.5177 - val_loss: 885.6941\n",
      "Epoch 49/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 1049.0790 - val_loss: 902.4914\n",
      "Epoch 50/500\n",
      "273/273 [==============================] - 0s 154us/step - loss: 1010.6844 - val_loss: 860.0250\n",
      "Epoch 51/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 1025.5173 - val_loss: 848.5963\n",
      "Epoch 52/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 989.9895 - val_loss: 838.5043\n",
      "Epoch 53/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 976.1541 - val_loss: 930.3634\n",
      "Epoch 54/500\n",
      "273/273 [==============================] - 0s 156us/step - loss: 1036.5065 - val_loss: 819.2382\n",
      "Epoch 55/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 979.5152 - val_loss: 826.3178\n",
      "Epoch 56/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 939.3943 - val_loss: 834.3970\n",
      "Epoch 57/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 920.0580 - val_loss: 830.7231\n",
      "Epoch 58/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 973.6266 - val_loss: 832.2619\n",
      "Epoch 59/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 970.4518 - val_loss: 800.5226\n",
      "Epoch 60/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 1025.6796 - val_loss: 809.9231\n",
      "Epoch 61/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 1021.8743 - val_loss: 813.4162\n",
      "Epoch 62/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 941.0224 - val_loss: 784.1929\n",
      "Epoch 63/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 974.9035 - val_loss: 776.1922\n",
      "Epoch 64/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 969.4227 - val_loss: 798.6686\n",
      "Epoch 65/500\n",
      "273/273 [==============================] - 0s 169us/step - loss: 961.7366 - val_loss: 771.7703\n",
      "Epoch 66/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 942.6640 - val_loss: 763.2854\n",
      "Epoch 67/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 854.9072 - val_loss: 830.8394\n",
      "Epoch 68/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 885.0065 - val_loss: 761.3103\n",
      "Epoch 69/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 979.7769 - val_loss: 758.0998\n",
      "Epoch 70/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 889.8417 - val_loss: 735.1836\n",
      "Epoch 71/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 916.8690 - val_loss: 746.1290\n",
      "Epoch 72/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 936.7847 - val_loss: 742.8138\n",
      "Epoch 73/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 927.1209 - val_loss: 740.2850\n",
      "Epoch 74/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 885.0369 - val_loss: 727.3279\n",
      "Epoch 75/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 959.5059 - val_loss: 722.0986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 934.6500 - val_loss: 699.9034\n",
      "Epoch 77/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 842.8664 - val_loss: 710.3497\n",
      "Epoch 78/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 921.1782 - val_loss: 733.6592\n",
      "Epoch 79/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 858.9757 - val_loss: 812.6290\n",
      "Epoch 80/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 1054.4812 - val_loss: 727.3053\n",
      "Epoch 81/500\n",
      "273/273 [==============================] - 0s 123us/step - loss: 888.7339 - val_loss: 720.9947\n",
      "Epoch 82/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 893.3933 - val_loss: 755.9539\n",
      "Epoch 83/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 931.1101 - val_loss: 717.6231\n",
      "Epoch 84/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 1019.4834 - val_loss: 711.0413\n",
      "Epoch 85/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 939.8029 - val_loss: 701.1339\n",
      "Epoch 86/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 900.1419 - val_loss: 723.2229\n",
      "Epoch 87/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 869.6170 - val_loss: 687.3552\n",
      "Epoch 88/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 924.1805 - val_loss: 704.6751\n",
      "Epoch 89/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 900.9455 - val_loss: 693.5710\n",
      "Epoch 90/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 900.5269 - val_loss: 676.7536\n",
      "Epoch 91/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 832.3278 - val_loss: 680.4964\n",
      "Epoch 92/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 935.3716 - val_loss: 686.3164\n",
      "Epoch 93/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 932.8364 - val_loss: 725.6813\n",
      "Epoch 94/500\n",
      "273/273 [==============================] - 0s 169us/step - loss: 964.2175 - val_loss: 723.2480\n",
      "Epoch 95/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 844.1075 - val_loss: 710.4940\n",
      "Epoch 96/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 830.1814 - val_loss: 656.0428\n",
      "Epoch 97/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 900.8748 - val_loss: 715.3492\n",
      "Epoch 98/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 921.1668 - val_loss: 695.3991\n",
      "Epoch 99/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 881.1724 - val_loss: 659.4901\n",
      "Epoch 100/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 758.1062 - val_loss: 702.8479\n",
      "Epoch 101/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 895.8029 - val_loss: 698.2778\n",
      "Epoch 102/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 824.7182 - val_loss: 664.2305\n",
      "Epoch 103/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 847.2396 - val_loss: 697.6388\n",
      "Epoch 104/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 853.0537 - val_loss: 762.7198\n",
      "Epoch 105/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 862.4462 - val_loss: 712.2463\n",
      "Epoch 106/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 842.0793 - val_loss: 646.4068\n",
      "Epoch 107/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 914.6295 - val_loss: 634.6686\n",
      "Epoch 108/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 826.5286 - val_loss: 677.5684\n",
      "Epoch 109/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 877.5670 - val_loss: 666.8535\n",
      "Epoch 110/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 754.7112 - val_loss: 658.6481\n",
      "Epoch 111/500\n",
      "273/273 [==============================] - 0s 164us/step - loss: 774.5548 - val_loss: 649.8077\n",
      "Epoch 112/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 814.7725 - val_loss: 652.3334\n",
      "Epoch 113/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 892.3697 - val_loss: 627.0437\n",
      "Epoch 114/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 871.9409 - val_loss: 646.8377\n",
      "Epoch 115/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 767.8146 - val_loss: 635.3208\n",
      "Epoch 116/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 946.5102 - val_loss: 644.2672\n",
      "Epoch 117/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 806.9165 - val_loss: 649.0215\n",
      "Epoch 118/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 845.4962 - val_loss: 642.0131\n",
      "Epoch 119/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 822.8127 - val_loss: 664.1110\n",
      "Epoch 120/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 881.4225 - val_loss: 657.3496\n",
      "Epoch 121/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 917.8019 - val_loss: 662.6266\n",
      "Epoch 122/500\n",
      "273/273 [==============================] - 0s 162us/step - loss: 832.5420 - val_loss: 668.9318\n",
      "Epoch 123/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 857.9988 - val_loss: 654.5430\n",
      "Epoch 124/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 737.2107 - val_loss: 648.9034\n",
      "Epoch 125/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 801.1422 - val_loss: 675.4427\n",
      "Epoch 126/500\n",
      "273/273 [==============================] - 0s 158us/step - loss: 805.9690 - val_loss: 666.0328\n",
      "Epoch 127/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 797.3747 - val_loss: 636.3812\n",
      "Epoch 128/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 805.8032 - val_loss: 664.8469\n",
      "Epoch 129/500\n",
      "273/273 [==============================] - 0s 149us/step - loss: 827.8803 - val_loss: 630.3936\n",
      "Epoch 130/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 790.2621 - val_loss: 650.0357\n",
      "Epoch 131/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 777.8234 - val_loss: 677.2483\n",
      "Epoch 132/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 776.5181 - val_loss: 687.6124\n",
      "Epoch 133/500\n",
      "273/273 [==============================] - 0s 137us/step - loss: 846.0601 - val_loss: 687.0357\n",
      "Epoch 134/500\n",
      "273/273 [==============================] - 0s 147us/step - loss: 864.2933 - val_loss: 698.0093\n",
      "Epoch 135/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 778.9843 - val_loss: 641.1319\n",
      "Epoch 136/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 814.5192 - val_loss: 630.9456\n",
      "Epoch 137/500\n",
      "273/273 [==============================] - 0s 160us/step - loss: 796.6375 - val_loss: 623.6898\n",
      "Epoch 138/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 799.7604 - val_loss: 627.5524\n",
      "Epoch 139/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 795.8058 - val_loss: 621.3418\n",
      "Epoch 140/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 689.1804 - val_loss: 629.2141\n",
      "Epoch 141/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 743.8498 - val_loss: 640.5161\n",
      "Epoch 142/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 803.0948 - val_loss: 631.6831\n",
      "Epoch 143/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 742.9977 - val_loss: 645.3668\n",
      "Epoch 144/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 869.2811 - val_loss: 624.7044\n",
      "Epoch 145/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 842.7044 - val_loss: 627.7615\n",
      "Epoch 146/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 797.2073 - val_loss: 656.7108\n",
      "Epoch 147/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 781.8368 - val_loss: 616.6711\n",
      "Epoch 148/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 676.4047 - val_loss: 610.7798\n",
      "Epoch 149/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 767.6782 - val_loss: 619.8252\n",
      "Epoch 150/500\n",
      "273/273 [==============================] - 0s 162us/step - loss: 808.5189 - val_loss: 638.5681\n",
      "Epoch 151/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 811.8107 - val_loss: 658.3858\n",
      "Epoch 152/500\n",
      "273/273 [==============================] - 0s 160us/step - loss: 775.4241 - val_loss: 613.3514\n",
      "Epoch 153/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 732.1365 - val_loss: 640.7552\n",
      "Epoch 154/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 727.7035 - val_loss: 609.0995\n",
      "Epoch 155/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 791.1351 - val_loss: 600.4082\n",
      "Epoch 156/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 763.6357 - val_loss: 625.4930\n",
      "Epoch 157/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 854.7832 - val_loss: 631.1636\n",
      "Epoch 158/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 801.6961 - val_loss: 625.8048\n",
      "Epoch 159/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 835.2618 - val_loss: 655.4075\n",
      "Epoch 160/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 778.5351 - val_loss: 639.7878\n",
      "Epoch 161/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 698.3701 - val_loss: 630.4264\n",
      "Epoch 162/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 778.9754 - val_loss: 630.2109\n",
      "Epoch 163/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 757.6819 - val_loss: 631.2375\n",
      "Epoch 164/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 734.6710 - val_loss: 637.2545\n",
      "Epoch 165/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 751.8658 - val_loss: 632.9479\n",
      "Epoch 166/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 800.7191 - val_loss: 651.2002\n",
      "Epoch 167/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 857.1378 - val_loss: 691.2001\n",
      "Epoch 168/500\n",
      "273/273 [==============================] - 0s 156us/step - loss: 805.7582 - val_loss: 659.6948\n",
      "Epoch 169/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 762.1400 - val_loss: 611.6791\n",
      "Epoch 170/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 714.0290 - val_loss: 641.4976\n",
      "Epoch 171/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 844.5606 - val_loss: 634.8254\n",
      "Epoch 172/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 784.2527 - val_loss: 677.8439\n",
      "Epoch 173/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 798.8782 - val_loss: 697.6452\n",
      "Epoch 174/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 827.7369 - val_loss: 658.0490\n",
      "Epoch 175/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 761.2146 - val_loss: 625.1837\n",
      "Epoch 176/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 774.9665 - val_loss: 627.0767\n",
      "Epoch 177/500\n",
      "273/273 [==============================] - 0s 160us/step - loss: 740.0193 - val_loss: 624.2799\n",
      "Epoch 178/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 713.0267 - val_loss: 623.3992\n",
      "Epoch 179/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 853.2756 - val_loss: 646.6028\n",
      "Epoch 180/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 728.9904 - val_loss: 643.2562\n",
      "Epoch 181/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 767.3184 - val_loss: 638.0749\n",
      "Epoch 182/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 735.0710 - val_loss: 616.5264\n",
      "Epoch 183/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 805.2127 - val_loss: 699.6312\n",
      "Epoch 184/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 875.3203 - val_loss: 694.9675\n",
      "Epoch 185/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 830.5810 - val_loss: 639.4772\n",
      "Epoch 186/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 715.3603 - val_loss: 602.9127\n",
      "Epoch 187/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 759.9554 - val_loss: 639.8047\n",
      "Epoch 188/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 753.4643 - val_loss: 628.6352\n",
      "Epoch 189/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 812.5517 - val_loss: 629.8126\n",
      "Epoch 190/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 755.2204 - val_loss: 605.6362\n",
      "Epoch 191/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 703.6567 - val_loss: 614.1499\n",
      "Epoch 192/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 691.2176 - val_loss: 603.6641\n",
      "Epoch 193/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 738.8380 - val_loss: 588.2603\n",
      "Epoch 194/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 737.5581 - val_loss: 579.8384\n",
      "Epoch 195/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 706.1616 - val_loss: 629.6018\n",
      "Epoch 196/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 716.6680 - val_loss: 584.4053\n",
      "Epoch 197/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 701.2683 - val_loss: 600.6085\n",
      "Epoch 198/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 723.8027 - val_loss: 608.1434\n",
      "Epoch 199/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 672.7079 - val_loss: 609.8942\n",
      "Epoch 200/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 779.9631 - val_loss: 609.7768\n",
      "Epoch 201/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 745.2616 - val_loss: 602.6839\n",
      "Epoch 202/500\n",
      "273/273 [==============================] - 0s 118us/step - loss: 754.3137 - val_loss: 607.4141\n",
      "Epoch 203/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 721.0590 - val_loss: 590.6837\n",
      "Epoch 204/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 683.2342 - val_loss: 590.6170\n",
      "Epoch 205/500\n",
      "273/273 [==============================] - 0s 149us/step - loss: 668.8879 - val_loss: 583.8538\n",
      "Epoch 206/500\n",
      "273/273 [==============================] - 0s 164us/step - loss: 778.1344 - val_loss: 663.2937\n",
      "Epoch 207/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 761.3020 - val_loss: 604.5889\n",
      "Epoch 208/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 755.8143 - val_loss: 624.9370\n",
      "Epoch 209/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 703.0764 - val_loss: 610.9183\n",
      "Epoch 210/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 705.8745 - val_loss: 597.6221\n",
      "Epoch 211/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 742.5156 - val_loss: 597.8744\n",
      "Epoch 212/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 727.1996 - val_loss: 650.3738\n",
      "Epoch 213/500\n",
      "273/273 [==============================] - 0s 149us/step - loss: 704.3801 - val_loss: 595.9675\n",
      "Epoch 214/500\n",
      "273/273 [==============================] - 0s 118us/step - loss: 691.8533 - val_loss: 606.4584\n",
      "Epoch 215/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 815.9716 - val_loss: 610.2840\n",
      "Epoch 216/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 702.4685 - val_loss: 615.0503\n",
      "Epoch 217/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 745.2628 - val_loss: 568.8717\n",
      "Epoch 218/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 761.4210 - val_loss: 579.6245\n",
      "Epoch 219/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 721.8704 - val_loss: 624.8086\n",
      "Epoch 220/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 756.5922 - val_loss: 589.3275\n",
      "Epoch 221/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 722.2385 - val_loss: 604.9017\n",
      "Epoch 222/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 698.0458 - val_loss: 580.8409\n",
      "Epoch 223/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 763.8474 - val_loss: 583.4979\n",
      "Epoch 224/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 677.3146 - val_loss: 614.3055\n",
      "Epoch 225/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 701.6194 - val_loss: 606.1819\n",
      "Epoch 226/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 [==============================] - 0s 127us/step - loss: 678.3162 - val_loss: 601.2121\n",
      "Epoch 227/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 767.0100 - val_loss: 602.7376\n",
      "Epoch 228/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 707.5316 - val_loss: 622.4515\n",
      "Epoch 229/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 753.9437 - val_loss: 608.2306\n",
      "Epoch 230/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 689.8546 - val_loss: 587.7759\n",
      "Epoch 231/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 728.0499 - val_loss: 587.6912\n",
      "Epoch 232/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 725.3655 - val_loss: 588.1596\n",
      "Epoch 233/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 876.3560 - val_loss: 609.8109\n",
      "Epoch 234/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 716.9243 - val_loss: 632.1807\n",
      "Epoch 235/500\n",
      "273/273 [==============================] - 0s 156us/step - loss: 778.8498 - val_loss: 615.1133\n",
      "Epoch 236/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 762.6682 - val_loss: 606.2928\n",
      "Epoch 237/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 831.0587 - val_loss: 584.3975\n",
      "Epoch 238/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 755.0600 - val_loss: 570.3326\n",
      "Epoch 239/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 759.2726 - val_loss: 621.5806\n",
      "Epoch 240/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 817.2563 - val_loss: 649.6112\n",
      "Epoch 241/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 720.1198 - val_loss: 611.1028\n",
      "Epoch 242/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 729.0732 - val_loss: 592.8506\n",
      "Epoch 243/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 665.0030 - val_loss: 568.1321\n",
      "Epoch 244/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 682.5201 - val_loss: 581.3888\n",
      "Epoch 245/500\n",
      "273/273 [==============================] - 0s 123us/step - loss: 662.7396 - val_loss: 567.3421\n",
      "Epoch 246/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 674.8835 - val_loss: 582.9647\n",
      "Epoch 247/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 735.6620 - val_loss: 576.7067\n",
      "Epoch 248/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 668.3859 - val_loss: 609.6576\n",
      "Epoch 249/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 690.1890 - val_loss: 613.6573\n",
      "Epoch 250/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 733.9553 - val_loss: 642.1189\n",
      "Epoch 251/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 673.3818 - val_loss: 664.8293\n",
      "Epoch 252/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 840.8514 - val_loss: 638.8323\n",
      "Epoch 253/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 751.4734 - val_loss: 604.9270\n",
      "Epoch 254/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 647.4704 - val_loss: 597.7275\n",
      "Epoch 255/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 591.8212 - val_loss: 603.7488\n",
      "Epoch 256/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 712.7758 - val_loss: 611.7963\n",
      "Epoch 257/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 693.5022 - val_loss: 605.1570\n",
      "Epoch 258/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 660.2657 - val_loss: 624.0885\n",
      "Epoch 259/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 742.1502 - val_loss: 689.3518\n",
      "Epoch 260/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 723.1126 - val_loss: 644.1636\n",
      "Epoch 261/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 742.8365 - val_loss: 602.4099\n",
      "Epoch 262/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 692.0475 - val_loss: 625.2835\n",
      "Epoch 263/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 616.0113 - val_loss: 589.3357\n",
      "Epoch 264/500\n",
      "273/273 [==============================] - 0s 162us/step - loss: 695.9538 - val_loss: 589.8096\n",
      "Epoch 265/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 652.4207 - val_loss: 622.0908\n",
      "Epoch 266/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 695.0371 - val_loss: 646.8147\n",
      "Epoch 267/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 796.8724 - val_loss: 625.0359\n",
      "Epoch 268/500\n",
      "273/273 [==============================] - 0s 114us/step - loss: 745.8630 - val_loss: 629.0363\n",
      "Epoch 269/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 730.6490 - val_loss: 624.8481\n",
      "Epoch 270/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 717.3670 - val_loss: 613.6881\n",
      "Epoch 271/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 693.8529 - val_loss: 602.4268\n",
      "Epoch 272/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 700.5070 - val_loss: 612.4758\n",
      "Epoch 273/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 666.2701 - val_loss: 592.3355\n",
      "Epoch 274/500\n",
      "273/273 [==============================] - 0s 123us/step - loss: 615.2341 - val_loss: 601.6473\n",
      "Epoch 275/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 638.0409 - val_loss: 606.3968\n",
      "Epoch 276/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 686.5323 - val_loss: 598.3692\n",
      "Epoch 277/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 758.7293 - val_loss: 622.6943\n",
      "Epoch 278/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 686.6538 - val_loss: 602.4027\n",
      "Epoch 279/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 683.7868 - val_loss: 579.3551\n",
      "Epoch 280/500\n",
      "273/273 [==============================] - 0s 114us/step - loss: 700.6701 - val_loss: 599.2220\n",
      "Epoch 281/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 694.2735 - val_loss: 623.2721\n",
      "Epoch 282/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 755.1372 - val_loss: 635.1976\n",
      "Epoch 283/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 659.2635 - val_loss: 598.3813\n",
      "Epoch 284/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 681.9303 - val_loss: 604.3508\n",
      "Epoch 285/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 676.9809 - val_loss: 586.1297\n",
      "Epoch 286/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 620.7233 - val_loss: 609.0265\n",
      "Epoch 287/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 731.9792 - val_loss: 593.9404\n",
      "Epoch 288/500\n",
      "273/273 [==============================] - 0s 126us/step - loss: 741.0297 - val_loss: 630.0793\n",
      "Epoch 289/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 704.6573 - val_loss: 659.9781\n",
      "Epoch 290/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 737.2210 - val_loss: 597.0869\n",
      "Epoch 291/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 746.9465 - val_loss: 618.5813\n",
      "Epoch 292/500\n",
      "273/273 [==============================] - 0s 165us/step - loss: 669.9528 - val_loss: 604.3738\n",
      "Epoch 293/500\n",
      "273/273 [==============================] - 0s 143us/step - loss: 629.7538 - val_loss: 591.9881\n",
      "Epoch 294/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 647.5359 - val_loss: 585.5826\n",
      "Epoch 295/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 679.9212 - val_loss: 582.4975\n",
      "Epoch 296/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 682.7112 - val_loss: 596.4447\n",
      "Epoch 297/500\n",
      "273/273 [==============================] - 0s 143us/step - loss: 660.6252 - val_loss: 579.4994\n",
      "Epoch 298/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 691.5671 - val_loss: 609.7507\n",
      "Epoch 299/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 778.8719 - val_loss: 589.7359\n",
      "Epoch 300/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 666.9557 - val_loss: 621.1571\n",
      "Epoch 301/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 723.9565 - val_loss: 615.3799\n",
      "Epoch 302/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 713.3619 - val_loss: 605.7903\n",
      "Epoch 303/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 696.9639 - val_loss: 588.0871\n",
      "Epoch 304/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 651.6260 - val_loss: 577.1929\n",
      "Epoch 305/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 689.4198 - val_loss: 585.5729\n",
      "Epoch 306/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 708.7541 - val_loss: 586.4879\n",
      "Epoch 307/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 620.5315 - val_loss: 631.5356\n",
      "Epoch 308/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 626.0160 - val_loss: 580.5852\n",
      "Epoch 309/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 602.9266 - val_loss: 577.0673\n",
      "Epoch 310/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 636.8205 - val_loss: 578.6497\n",
      "Epoch 311/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 632.2136 - val_loss: 583.0047\n",
      "Epoch 312/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 659.6660 - val_loss: 603.9435\n",
      "Epoch 313/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 649.4437 - val_loss: 596.1647\n",
      "Epoch 314/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 748.7313 - val_loss: 593.0203\n",
      "Epoch 315/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 710.3630 - val_loss: 575.4591\n",
      "Epoch 316/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 612.1030 - val_loss: 630.9411\n",
      "Epoch 317/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 616.6660 - val_loss: 597.1330\n",
      "Epoch 318/500\n",
      "273/273 [==============================] - 0s 132us/step - loss: 655.0988 - val_loss: 607.7634\n",
      "Epoch 319/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 594.8406 - val_loss: 594.3279\n",
      "Epoch 320/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 725.5319 - val_loss: 588.5589\n",
      "Epoch 321/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 723.6205 - val_loss: 582.5569\n",
      "Epoch 322/500\n",
      "273/273 [==============================] - 0s 162us/step - loss: 708.0799 - val_loss: 590.2682\n",
      "Epoch 323/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 706.3753 - val_loss: 586.5567\n",
      "Epoch 324/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 721.3688 - val_loss: 634.0755\n",
      "Epoch 325/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 652.0250 - val_loss: 652.8521\n",
      "Epoch 326/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 651.6265 - val_loss: 615.6334\n",
      "Epoch 327/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 719.0619 - val_loss: 596.2034\n",
      "Epoch 328/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 664.4605 - val_loss: 583.2732\n",
      "Epoch 329/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 682.5638 - val_loss: 586.3143\n",
      "Epoch 330/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 658.3849 - val_loss: 589.2626\n",
      "Epoch 331/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 712.1581 - val_loss: 589.1428\n",
      "Epoch 332/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 713.1465 - val_loss: 587.4667\n",
      "Epoch 333/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 636.1236 - val_loss: 605.8144\n",
      "Epoch 334/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 663.2639 - val_loss: 592.4858\n",
      "Epoch 335/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 640.4446 - val_loss: 621.6883\n",
      "Epoch 336/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 668.5996 - val_loss: 578.5569\n",
      "Epoch 337/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 650.5870 - val_loss: 571.0607\n",
      "Epoch 338/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 646.3257 - val_loss: 582.2451\n",
      "Epoch 339/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 653.1108 - val_loss: 621.0868\n",
      "Epoch 340/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 633.6225 - val_loss: 602.8608\n",
      "Epoch 341/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 600.9225 - val_loss: 603.3234\n",
      "Epoch 342/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 674.8564 - val_loss: 591.1556\n",
      "Epoch 343/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 704.4436 - val_loss: 579.0197\n",
      "Epoch 344/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 696.7552 - val_loss: 596.1061\n",
      "Epoch 345/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 693.5569 - val_loss: 617.1885\n",
      "Epoch 346/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 663.8472 - val_loss: 600.2888\n",
      "Epoch 347/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 673.3057 - val_loss: 578.3057\n",
      "Epoch 348/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 620.1983 - val_loss: 576.3044\n",
      "Epoch 349/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 604.2589 - val_loss: 595.1587\n",
      "Epoch 350/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 667.0929 - val_loss: 622.9890\n",
      "Epoch 351/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 642.5442 - val_loss: 591.5243\n",
      "Epoch 352/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 618.4815 - val_loss: 573.0027\n",
      "Epoch 353/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 672.3481 - val_loss: 571.4066\n",
      "Epoch 354/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 649.1876 - val_loss: 596.6011\n",
      "Epoch 355/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 620.6008 - val_loss: 626.5084\n",
      "Epoch 356/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 686.6546 - val_loss: 671.1882\n",
      "Epoch 357/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 699.5645 - val_loss: 575.4604\n",
      "Epoch 358/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 666.3406 - val_loss: 636.7573\n",
      "Epoch 359/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 711.0138 - val_loss: 589.2890\n",
      "Epoch 360/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 606.4125 - val_loss: 582.3105\n",
      "Epoch 361/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 673.3571 - val_loss: 584.2566\n",
      "Epoch 362/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 614.8701 - val_loss: 607.2266\n",
      "Epoch 363/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 616.2040 - val_loss: 615.6160\n",
      "Epoch 364/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 657.8175 - val_loss: 614.3676\n",
      "Epoch 365/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 664.6515 - val_loss: 588.6938\n",
      "Epoch 366/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 703.9163 - val_loss: 583.1192\n",
      "Epoch 367/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 644.7360 - val_loss: 583.4649\n",
      "Epoch 368/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 654.9158 - val_loss: 582.1216\n",
      "Epoch 369/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 609.7043 - val_loss: 593.0506\n",
      "Epoch 370/500\n",
      "273/273 [==============================] - 0s 147us/step - loss: 606.0372 - val_loss: 658.8207\n",
      "Epoch 371/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 677.9801 - val_loss: 640.7450\n",
      "Epoch 372/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 721.8994 - val_loss: 613.2037\n",
      "Epoch 373/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 641.0836 - val_loss: 585.8836\n",
      "Epoch 374/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 669.3944 - val_loss: 617.3351\n",
      "Epoch 375/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 656.6516 - val_loss: 641.0143\n",
      "Epoch 376/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 [==============================] - 0s 129us/step - loss: 658.3686 - val_loss: 573.2977\n",
      "Epoch 377/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 658.7777 - val_loss: 576.6233\n",
      "Epoch 378/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 667.2170 - val_loss: 656.5293\n",
      "Epoch 379/500\n",
      "273/273 [==============================] - 0s 151us/step - loss: 700.1768 - val_loss: 618.6350\n",
      "Epoch 380/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 653.3812 - val_loss: 603.7752\n",
      "Epoch 381/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 674.1141 - val_loss: 579.6132\n",
      "Epoch 382/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 665.6771 - val_loss: 586.2812\n",
      "Epoch 383/500\n",
      "273/273 [==============================] - 0s 118us/step - loss: 605.3919 - val_loss: 558.8220\n",
      "Epoch 384/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 577.0457 - val_loss: 573.4101\n",
      "Epoch 385/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 688.3260 - val_loss: 570.1121\n",
      "Epoch 386/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 641.5416 - val_loss: 557.1203\n",
      "Epoch 387/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 697.9205 - val_loss: 570.9712\n",
      "Epoch 388/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 634.5301 - val_loss: 593.0580\n",
      "Epoch 389/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 669.5505 - val_loss: 571.8740\n",
      "Epoch 390/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 657.7835 - val_loss: 585.5243\n",
      "Epoch 391/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 682.1491 - val_loss: 609.9956\n",
      "Epoch 392/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 676.7981 - val_loss: 581.1417\n",
      "Epoch 393/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 577.6080 - val_loss: 576.1580\n",
      "Epoch 394/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 630.0945 - val_loss: 642.9246\n",
      "Epoch 395/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 678.4019 - val_loss: 640.5775\n",
      "Epoch 396/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 651.2022 - val_loss: 616.3810\n",
      "Epoch 397/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 718.1716 - val_loss: 575.2303\n",
      "Epoch 398/500\n",
      "273/273 [==============================] - 0s 149us/step - loss: 624.4995 - val_loss: 574.5887\n",
      "Epoch 399/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 639.3895 - val_loss: 626.7026\n",
      "Epoch 400/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 747.7774 - val_loss: 582.4296\n",
      "Epoch 401/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 641.2383 - val_loss: 597.4566\n",
      "Epoch 402/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 690.8814 - val_loss: 611.9423\n",
      "Epoch 403/500\n",
      "273/273 [==============================] - 0s 145us/step - loss: 614.3786 - val_loss: 604.2828\n",
      "Epoch 404/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 653.5840 - val_loss: 584.4048\n",
      "Epoch 405/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 676.8309 - val_loss: 589.6816\n",
      "Epoch 406/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 592.9502 - val_loss: 599.0062\n",
      "Epoch 407/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 677.7011 - val_loss: 601.9923\n",
      "Epoch 408/500\n",
      "273/273 [==============================] - 0s 162us/step - loss: 610.5138 - val_loss: 598.3933\n",
      "Epoch 409/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 644.8140 - val_loss: 601.6898\n",
      "Epoch 410/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 635.3799 - val_loss: 606.9839\n",
      "Epoch 411/500\n",
      "273/273 [==============================] - 0s 144us/step - loss: 604.0698 - val_loss: 599.0794\n",
      "Epoch 412/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 571.1956 - val_loss: 576.0440\n",
      "Epoch 413/500\n",
      "273/273 [==============================] - 0s 149us/step - loss: 637.0927 - val_loss: 568.6371\n",
      "Epoch 414/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 674.5896 - val_loss: 572.9500\n",
      "Epoch 415/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 623.6035 - val_loss: 569.2196\n",
      "Epoch 416/500\n",
      "273/273 [==============================] - 0s 146us/step - loss: 625.5335 - val_loss: 601.4313\n",
      "Epoch 417/500\n",
      "273/273 [==============================] - 0s 134us/step - loss: 599.0286 - val_loss: 596.7687\n",
      "Epoch 418/500\n",
      "273/273 [==============================] - 0s 147us/step - loss: 568.0511 - val_loss: 635.0998\n",
      "Epoch 419/500\n",
      "273/273 [==============================] - 0s 147us/step - loss: 634.8767 - val_loss: 630.8641\n",
      "Epoch 420/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 617.8189 - val_loss: 589.6634\n",
      "Epoch 421/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 592.3160 - val_loss: 566.0074\n",
      "Epoch 422/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 641.3399 - val_loss: 607.3560\n",
      "Epoch 423/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 615.5436 - val_loss: 649.2893\n",
      "Epoch 424/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 636.9556 - val_loss: 566.1621\n",
      "Epoch 425/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 648.7619 - val_loss: 618.6637\n",
      "Epoch 426/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 694.7661 - val_loss: 668.2321\n",
      "Epoch 427/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 675.2836 - val_loss: 698.4901\n",
      "Epoch 428/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 675.5679 - val_loss: 605.4444\n",
      "Epoch 429/500\n",
      "273/273 [==============================] - 0s 118us/step - loss: 629.7112 - val_loss: 598.2207\n",
      "Epoch 430/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 658.8571 - val_loss: 589.9965\n",
      "Epoch 431/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 616.1563 - val_loss: 597.1466\n",
      "Epoch 432/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 604.2087 - val_loss: 631.7832\n",
      "Epoch 433/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 641.8964 - val_loss: 611.1445\n",
      "Epoch 434/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 649.5376 - val_loss: 605.4437\n",
      "Epoch 435/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 655.0848 - val_loss: 600.6260\n",
      "Epoch 436/500\n",
      "273/273 [==============================] - 0s 165us/step - loss: 633.7671 - val_loss: 582.2971\n",
      "Epoch 437/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 596.8110 - val_loss: 590.9823\n",
      "Epoch 438/500\n",
      "273/273 [==============================] - 0s 153us/step - loss: 622.2275 - val_loss: 635.7700\n",
      "Epoch 439/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 668.0589 - val_loss: 574.9331\n",
      "Epoch 440/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 657.8950 - val_loss: 587.4370\n",
      "Epoch 441/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 626.9041 - val_loss: 583.2990\n",
      "Epoch 442/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 662.7764 - val_loss: 584.5240\n",
      "Epoch 443/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 553.9474 - val_loss: 571.4239\n",
      "Epoch 444/500\n",
      "273/273 [==============================] - 0s 120us/step - loss: 623.4161 - val_loss: 593.2473\n",
      "Epoch 445/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 636.3574 - val_loss: 578.8128\n",
      "Epoch 446/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 599.7147 - val_loss: 598.9496\n",
      "Epoch 447/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 602.1532 - val_loss: 609.3692\n",
      "Epoch 448/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 621.6805 - val_loss: 574.4074\n",
      "Epoch 449/500\n",
      "273/273 [==============================] - 0s 142us/step - loss: 560.3863 - val_loss: 597.0770\n",
      "Epoch 450/500\n",
      "273/273 [==============================] - 0s 118us/step - loss: 635.1815 - val_loss: 601.6739\n",
      "Epoch 451/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 625.1754 - val_loss: 652.0134\n",
      "Epoch 452/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 642.1147 - val_loss: 605.4037\n",
      "Epoch 453/500\n",
      "273/273 [==============================] - 0s 118us/step - loss: 651.0876 - val_loss: 585.3833\n",
      "Epoch 454/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 624.6001 - val_loss: 602.8516\n",
      "Epoch 455/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 597.6350 - val_loss: 586.5042\n",
      "Epoch 456/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 583.9453 - val_loss: 575.6677\n",
      "Epoch 457/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 627.5708 - val_loss: 556.8512\n",
      "Epoch 458/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 567.8250 - val_loss: 577.0548\n",
      "Epoch 459/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 675.3362 - val_loss: 597.3452\n",
      "Epoch 460/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 584.8167 - val_loss: 596.2364\n",
      "Epoch 461/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 632.1263 - val_loss: 583.1386\n",
      "Epoch 462/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 620.2200 - val_loss: 604.1743\n",
      "Epoch 463/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 577.9006 - val_loss: 588.7060\n",
      "Epoch 464/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 640.5007 - val_loss: 584.9004\n",
      "Epoch 465/500\n",
      "273/273 [==============================] - 0s 156us/step - loss: 635.9646 - val_loss: 584.6980\n",
      "Epoch 466/500\n",
      "273/273 [==============================] - 0s 158us/step - loss: 603.4999 - val_loss: 595.5966\n",
      "Epoch 467/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 580.9295 - val_loss: 632.8285\n",
      "Epoch 468/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 610.0176 - val_loss: 569.9324\n",
      "Epoch 469/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 556.0980 - val_loss: 577.9110\n",
      "Epoch 470/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 616.3911 - val_loss: 587.4484\n",
      "Epoch 471/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 596.7615 - val_loss: 631.9023\n",
      "Epoch 472/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 703.6201 - val_loss: 592.5636\n",
      "Epoch 473/500\n",
      "273/273 [==============================] - 0s 118us/step - loss: 632.0802 - val_loss: 594.7629\n",
      "Epoch 474/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 581.4982 - val_loss: 600.7798\n",
      "Epoch 475/500\n",
      "273/273 [==============================] - 0s 158us/step - loss: 619.7020 - val_loss: 617.5378\n",
      "Epoch 476/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 581.2725 - val_loss: 571.7033\n",
      "Epoch 477/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 615.9428 - val_loss: 611.0022\n",
      "Epoch 478/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 565.5704 - val_loss: 595.5500\n",
      "Epoch 479/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 589.8480 - val_loss: 593.6500\n",
      "Epoch 480/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 583.5537 - val_loss: 570.2906\n",
      "Epoch 481/500\n",
      "273/273 [==============================] - 0s 124us/step - loss: 600.3909 - val_loss: 576.9939\n",
      "Epoch 482/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 573.5789 - val_loss: 571.3828\n",
      "Epoch 483/500\n",
      "273/273 [==============================] - 0s 135us/step - loss: 660.6945 - val_loss: 588.4835\n",
      "Epoch 484/500\n",
      "273/273 [==============================] - 0s 136us/step - loss: 562.6045 - val_loss: 578.7752\n",
      "Epoch 485/500\n",
      "273/273 [==============================] - 0s 122us/step - loss: 642.0015 - val_loss: 587.3696\n",
      "Epoch 486/500\n",
      "273/273 [==============================] - 0s 131us/step - loss: 573.4823 - val_loss: 580.6517\n",
      "Epoch 487/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 590.9929 - val_loss: 586.2261\n",
      "Epoch 488/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 601.1528 - val_loss: 604.5059\n",
      "Epoch 489/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 602.5938 - val_loss: 578.0089\n",
      "Epoch 490/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 557.0699 - val_loss: 579.5627\n",
      "Epoch 491/500\n",
      "273/273 [==============================] - 0s 127us/step - loss: 591.7771 - val_loss: 586.5328\n",
      "Epoch 492/500\n",
      "273/273 [==============================] - 0s 116us/step - loss: 589.4731 - val_loss: 590.3487\n",
      "Epoch 493/500\n",
      "273/273 [==============================] - 0s 158us/step - loss: 550.7012 - val_loss: 575.0812\n",
      "Epoch 494/500\n",
      "273/273 [==============================] - 0s 129us/step - loss: 603.9047 - val_loss: 594.0529\n",
      "Epoch 495/500\n",
      "273/273 [==============================] - 0s 156us/step - loss: 573.5415 - val_loss: 581.5016\n",
      "Epoch 496/500\n",
      "273/273 [==============================] - 0s 125us/step - loss: 524.3655 - val_loss: 606.5259\n",
      "Epoch 497/500\n",
      "273/273 [==============================] - 0s 140us/step - loss: 577.2413 - val_loss: 589.9873\n",
      "Epoch 498/500\n",
      "273/273 [==============================] - 0s 138us/step - loss: 576.2179 - val_loss: 602.0490\n",
      "Epoch 499/500\n",
      "273/273 [==============================] - 0s 114us/step - loss: 583.4452 - val_loss: 590.6575\n",
      "Epoch 500/500\n",
      "273/273 [==============================] - 0s 133us/step - loss: 584.0471 - val_loss: 579.9379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d620d85710>"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2898877495445199"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, rec_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579.9378565448217"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rec_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_pos = ['CB', 'DE', 'ILB', 'S', 'DT', 'OLB', 'LB', 'DB', 'DL', 'NT']\n",
    "players['defense'] = players['position'].map(lambda x: 1 if x in def_pos else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(769, 64)"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defs = players[players['defense']==1][avg_cols]\n",
    "\n",
    "defs = pd.get_dummies(defs, columns=['conference'], drop_first=True)\n",
    "defs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = defs[defs['draft_year']<=2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school', 'position', 'team_nfl'])\n",
    "y_train = defs[defs['draft_year']<=2015]['avg_grade']\n",
    "X_test = defs[defs['draft_year']>2015].drop(columns=['avg_grade', 'draft_year', 'player', 'school', 'position', 'team_nfl'])\n",
    "y_test = defs[defs['draft_year']>2015]['avg_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0528048086091712"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "r2_score(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1539.1376682193973"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06451971836660919"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)\n",
    "rf_preds = rf_reg.predict(X_test)\n",
    "\n",
    "r2_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701.3978792820377"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "def_model = Sequential()\n",
    "def_model.add(Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'))\n",
    "def_model.add(Dropout(0.5))\n",
    "def_model.add(Dense(25, activation='relu'))\n",
    "# def_model.add(Dense(6, activation='relu'))\n",
    "def_model.add(Dense(1))\n",
    "\n",
    "adam = Adam(lr=0.01)\n",
    "def_model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 456 samples, validate on 313 samples\n",
      "Epoch 1/250\n",
      "456/456 [==============================] - 4s 9ms/step - loss: 1670.8036 - val_loss: 1121.2713\n",
      "Epoch 2/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 1144.2466 - val_loss: 1147.7180\n",
      "Epoch 3/250\n",
      "456/456 [==============================] - 0s 132us/step - loss: 1221.8587 - val_loss: 1219.7813\n",
      "Epoch 4/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 1091.5288 - val_loss: 1134.6304\n",
      "Epoch 5/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 1082.9625 - val_loss: 1113.6266\n",
      "Epoch 6/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 1130.4502 - val_loss: 1149.4626\n",
      "Epoch 7/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 1014.9361 - val_loss: 1326.8932\n",
      "Epoch 8/250\n",
      "456/456 [==============================] - 0s 127us/step - loss: 975.8780 - val_loss: 1236.5486\n",
      "Epoch 9/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 1007.8221 - val_loss: 1297.3675\n",
      "Epoch 10/250\n",
      "456/456 [==============================] - 0s 156us/step - loss: 965.0197 - val_loss: 1189.6935\n",
      "Epoch 11/250\n",
      "456/456 [==============================] - 0s 182us/step - loss: 987.3006 - val_loss: 1230.7729\n",
      "Epoch 12/250\n",
      "456/456 [==============================] - 0s 142us/step - loss: 861.7828 - val_loss: 1296.3299\n",
      "Epoch 13/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 999.7786 - val_loss: 1082.1774\n",
      "Epoch 14/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 956.0603 - val_loss: 1147.0362\n",
      "Epoch 15/250\n",
      "456/456 [==============================] - 0s 132us/step - loss: 901.2300 - val_loss: 1140.2184\n",
      "Epoch 16/250\n",
      "456/456 [==============================] - 0s 134us/step - loss: 853.4017 - val_loss: 1323.8809\n",
      "Epoch 17/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 868.8841 - val_loss: 1210.7973\n",
      "Epoch 18/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 979.9377 - val_loss: 1280.5794\n",
      "Epoch 19/250\n",
      "456/456 [==============================] - 0s 128us/step - loss: 849.3182 - val_loss: 1173.9700\n",
      "Epoch 20/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 883.1879 - val_loss: 1223.0219\n",
      "Epoch 21/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 884.3109 - val_loss: 1279.0737\n",
      "Epoch 22/250\n",
      "456/456 [==============================] - 0s 133us/step - loss: 869.0966 - val_loss: 1242.9603\n",
      "Epoch 23/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 821.4031 - val_loss: 1277.9137\n",
      "Epoch 24/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 808.8929 - val_loss: 1469.7275\n",
      "Epoch 25/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 813.9133 - val_loss: 1326.3579\n",
      "Epoch 26/250\n",
      "456/456 [==============================] - 0s 146us/step - loss: 891.7535 - val_loss: 1160.0772\n",
      "Epoch 27/250\n",
      "456/456 [==============================] - 0s 150us/step - loss: 815.4397 - val_loss: 1218.9838\n",
      "Epoch 28/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 837.3312 - val_loss: 1326.2490\n",
      "Epoch 29/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 926.5546 - val_loss: 1464.8151\n",
      "Epoch 30/250\n",
      "456/456 [==============================] - 0s 127us/step - loss: 845.7621 - val_loss: 1354.8230\n",
      "Epoch 31/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 809.6270 - val_loss: 1346.7862\n",
      "Epoch 32/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 795.0660 - val_loss: 1077.3873\n",
      "Epoch 33/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 807.0338 - val_loss: 1098.5132\n",
      "Epoch 34/250\n",
      "456/456 [==============================] - 0s 130us/step - loss: 753.0163 - val_loss: 1234.6933\n",
      "Epoch 35/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 846.5806 - val_loss: 1330.7615\n",
      "Epoch 36/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 788.5033 - val_loss: 1133.0010\n",
      "Epoch 37/250\n",
      "456/456 [==============================] - 0s 142us/step - loss: 794.3525 - val_loss: 1241.4151\n",
      "Epoch 38/250\n",
      "456/456 [==============================] - 0s 126us/step - loss: 798.7783 - val_loss: 1138.3050\n",
      "Epoch 39/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 760.3097 - val_loss: 1176.4717\n",
      "Epoch 40/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 768.8287 - val_loss: 1313.7344\n",
      "Epoch 41/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 723.7890 - val_loss: 1240.1565\n",
      "Epoch 42/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 757.8717 - val_loss: 1159.2396\n",
      "Epoch 43/250\n",
      "456/456 [==============================] - 0s 133us/step - loss: 707.7273 - val_loss: 1220.9389\n",
      "Epoch 44/250\n",
      "456/456 [==============================] - 0s 166us/step - loss: 728.0263 - val_loss: 1176.6657\n",
      "Epoch 45/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 767.0420 - val_loss: 1243.5443\n",
      "Epoch 46/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 709.4258 - val_loss: 1118.5652\n",
      "Epoch 47/250\n",
      "456/456 [==============================] - 0s 131us/step - loss: 719.3694 - val_loss: 1168.6741\n",
      "Epoch 48/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 738.0177 - val_loss: 1018.3395\n",
      "Epoch 49/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 726.5005 - val_loss: 1029.5817\n",
      "Epoch 50/250\n",
      "456/456 [==============================] - 0s 129us/step - loss: 699.2543 - val_loss: 1160.4526\n",
      "Epoch 51/250\n",
      "456/456 [==============================] - 0s 144us/step - loss: 685.0572 - val_loss: 1007.7390\n",
      "Epoch 52/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 676.7263 - val_loss: 1034.6385\n",
      "Epoch 53/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 692.7061 - val_loss: 1081.2436\n",
      "Epoch 54/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 731.1597 - val_loss: 1074.4091\n",
      "Epoch 55/250\n",
      "456/456 [==============================] - 0s 144us/step - loss: 676.4788 - val_loss: 1098.9759\n",
      "Epoch 56/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 687.5257 - val_loss: 1069.6880\n",
      "Epoch 57/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 729.5737 - val_loss: 1039.9634\n",
      "Epoch 58/250\n",
      "456/456 [==============================] - 0s 153us/step - loss: 695.3561 - val_loss: 982.9010\n",
      "Epoch 59/250\n",
      "456/456 [==============================] - 0s 156us/step - loss: 689.2363 - val_loss: 938.2821\n",
      "Epoch 60/250\n",
      "456/456 [==============================] - 0s 174us/step - loss: 723.7792 - val_loss: 939.5386\n",
      "Epoch 61/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 707.4472 - val_loss: 1055.3697\n",
      "Epoch 62/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 713.8528 - val_loss: 1062.4254\n",
      "Epoch 63/250\n",
      "456/456 [==============================] - 0s 132us/step - loss: 701.5032 - val_loss: 1195.5907\n",
      "Epoch 64/250\n",
      "456/456 [==============================] - 0s 133us/step - loss: 727.7289 - val_loss: 1037.5201\n",
      "Epoch 65/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 687.6172 - val_loss: 853.1967\n",
      "Epoch 66/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 694.0619 - val_loss: 940.3109\n",
      "Epoch 67/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 692.4128 - val_loss: 923.8757\n",
      "Epoch 68/250\n",
      "456/456 [==============================] - 0s 159us/step - loss: 693.7249 - val_loss: 1140.1731\n",
      "Epoch 69/250\n",
      "456/456 [==============================] - 0s 153us/step - loss: 729.3231 - val_loss: 1153.9879\n",
      "Epoch 70/250\n",
      "456/456 [==============================] - 0s 152us/step - loss: 692.5343 - val_loss: 956.8898\n",
      "Epoch 71/250\n",
      "456/456 [==============================] - 0s 163us/step - loss: 683.6674 - val_loss: 1026.0108\n",
      "Epoch 72/250\n",
      "456/456 [==============================] - 0s 160us/step - loss: 685.3849 - val_loss: 805.5848\n",
      "Epoch 73/250\n",
      "456/456 [==============================] - 0s 155us/step - loss: 691.8868 - val_loss: 985.8497\n",
      "Epoch 74/250\n",
      "456/456 [==============================] - 0s 158us/step - loss: 664.4882 - val_loss: 856.1686\n",
      "Epoch 75/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 699.3206 - val_loss: 960.2315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/250\n",
      "456/456 [==============================] - 0s 162us/step - loss: 675.9449 - val_loss: 1033.7833\n",
      "Epoch 77/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 667.9479 - val_loss: 1000.4009\n",
      "Epoch 78/250\n",
      "456/456 [==============================] - 0s 134us/step - loss: 627.0686 - val_loss: 1058.2171\n",
      "Epoch 79/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 667.2970 - val_loss: 895.9393\n",
      "Epoch 80/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 647.1739 - val_loss: 997.3950\n",
      "Epoch 81/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 652.1454 - val_loss: 1049.2314\n",
      "Epoch 82/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 695.3061 - val_loss: 813.4943\n",
      "Epoch 83/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 666.1259 - val_loss: 905.8168\n",
      "Epoch 84/250\n",
      "456/456 [==============================] - 0s 155us/step - loss: 681.8172 - val_loss: 797.9380\n",
      "Epoch 85/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 662.7568 - val_loss: 812.2796\n",
      "Epoch 86/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 695.4989 - val_loss: 853.2496\n",
      "Epoch 87/250\n",
      "456/456 [==============================] - 0s 146us/step - loss: 641.2454 - val_loss: 809.1846\n",
      "Epoch 88/250\n",
      "456/456 [==============================] - 0s 146us/step - loss: 640.4068 - val_loss: 828.3414\n",
      "Epoch 89/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 624.5666 - val_loss: 794.4787\n",
      "Epoch 90/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 658.2556 - val_loss: 775.5336\n",
      "Epoch 91/250\n",
      "456/456 [==============================] - 0s 172us/step - loss: 628.5518 - val_loss: 819.9250\n",
      "Epoch 92/250\n",
      "456/456 [==============================] - 0s 163us/step - loss: 631.9584 - val_loss: 881.2705\n",
      "Epoch 93/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 633.1936 - val_loss: 776.5217\n",
      "Epoch 94/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 631.6540 - val_loss: 793.3449\n",
      "Epoch 95/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 595.0242 - val_loss: 829.0739\n",
      "Epoch 96/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 628.2951 - val_loss: 827.4642\n",
      "Epoch 97/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 605.7784 - val_loss: 759.9544\n",
      "Epoch 98/250\n",
      "456/456 [==============================] - 0s 142us/step - loss: 611.8088 - val_loss: 731.2648\n",
      "Epoch 99/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 630.4341 - val_loss: 838.1651\n",
      "Epoch 100/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 609.1261 - val_loss: 732.8671\n",
      "Epoch 101/250\n",
      "456/456 [==============================] - 0s 150us/step - loss: 631.9774 - val_loss: 777.5523\n",
      "Epoch 102/250\n",
      "456/456 [==============================] - 0s 128us/step - loss: 613.8709 - val_loss: 764.0205\n",
      "Epoch 103/250\n",
      "456/456 [==============================] - 0s 145us/step - loss: 618.0457 - val_loss: 770.3986\n",
      "Epoch 104/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 610.5244 - val_loss: 755.4491\n",
      "Epoch 105/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 599.7798 - val_loss: 752.1846\n",
      "Epoch 106/250\n",
      "456/456 [==============================] - 0s 144us/step - loss: 626.3456 - val_loss: 834.9742\n",
      "Epoch 107/250\n",
      "456/456 [==============================] - 0s 172us/step - loss: 619.7195 - val_loss: 827.6486\n",
      "Epoch 108/250\n",
      "456/456 [==============================] - 0s 144us/step - loss: 633.1098 - val_loss: 743.4799\n",
      "Epoch 109/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 614.4398 - val_loss: 744.6027\n",
      "Epoch 110/250\n",
      "456/456 [==============================] - 0s 146us/step - loss: 594.1853 - val_loss: 707.7990\n",
      "Epoch 111/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 619.2465 - val_loss: 733.1170\n",
      "Epoch 112/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 605.9262 - val_loss: 714.6835\n",
      "Epoch 113/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 615.5439 - val_loss: 749.5227\n",
      "Epoch 114/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 605.5926 - val_loss: 764.8202\n",
      "Epoch 115/250\n",
      "456/456 [==============================] - 0s 134us/step - loss: 610.0039 - val_loss: 729.5588\n",
      "Epoch 116/250\n",
      "456/456 [==============================] - 0s 145us/step - loss: 620.5574 - val_loss: 738.3335\n",
      "Epoch 117/250\n",
      "456/456 [==============================] - 0s 131us/step - loss: 603.2656 - val_loss: 724.7795\n",
      "Epoch 118/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 627.9178 - val_loss: 747.8226\n",
      "Epoch 119/250\n",
      "456/456 [==============================] - 0s 126us/step - loss: 603.3543 - val_loss: 777.0676\n",
      "Epoch 120/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 617.6000 - val_loss: 755.6292\n",
      "Epoch 121/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 641.4850 - val_loss: 699.2507\n",
      "Epoch 122/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 627.0255 - val_loss: 727.1511\n",
      "Epoch 123/250\n",
      "456/456 [==============================] - 0s 160us/step - loss: 616.7736 - val_loss: 732.0307\n",
      "Epoch 124/250\n",
      "456/456 [==============================] - 0s 152us/step - loss: 595.2268 - val_loss: 739.3735\n",
      "Epoch 125/250\n",
      "456/456 [==============================] - 0s 133us/step - loss: 608.7993 - val_loss: 758.5354\n",
      "Epoch 126/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 585.5363 - val_loss: 717.5044\n",
      "Epoch 127/250\n",
      "456/456 [==============================] - 0s 146us/step - loss: 605.9096 - val_loss: 741.2115\n",
      "Epoch 128/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 626.9734 - val_loss: 783.0540\n",
      "Epoch 129/250\n",
      "456/456 [==============================] - 0s 133us/step - loss: 600.9237 - val_loss: 682.5909\n",
      "Epoch 130/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 628.2942 - val_loss: 717.6999\n",
      "Epoch 131/250\n",
      "456/456 [==============================] - 0s 132us/step - loss: 637.3721 - val_loss: 715.4492\n",
      "Epoch 132/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 616.3946 - val_loss: 708.7978\n",
      "Epoch 133/250\n",
      "456/456 [==============================] - 0s 160us/step - loss: 621.8923 - val_loss: 706.1380\n",
      "Epoch 134/250\n",
      "456/456 [==============================] - 0s 157us/step - loss: 622.2025 - val_loss: 758.6583\n",
      "Epoch 135/250\n",
      "456/456 [==============================] - 0s 153us/step - loss: 599.4726 - val_loss: 715.3049\n",
      "Epoch 136/250\n",
      "456/456 [==============================] - 0s 152us/step - loss: 587.0336 - val_loss: 679.5827\n",
      "Epoch 137/250\n",
      "456/456 [==============================] - 0s 146us/step - loss: 592.8479 - val_loss: 729.5474\n",
      "Epoch 138/250\n",
      "456/456 [==============================] - 0s 150us/step - loss: 610.8926 - val_loss: 770.9624\n",
      "Epoch 139/250\n",
      "456/456 [==============================] - 0s 198us/step - loss: 612.9496 - val_loss: 744.4648\n",
      "Epoch 140/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 594.6438 - val_loss: 704.8468\n",
      "Epoch 141/250\n",
      "456/456 [==============================] - 0s 131us/step - loss: 577.4745 - val_loss: 721.1491\n",
      "Epoch 142/250\n",
      "456/456 [==============================] - 0s 155us/step - loss: 603.5985 - val_loss: 695.3573\n",
      "Epoch 143/250\n",
      "456/456 [==============================] - 0s 155us/step - loss: 632.5837 - val_loss: 714.1067\n",
      "Epoch 144/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 613.4163 - val_loss: 739.4037\n",
      "Epoch 145/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 584.2820 - val_loss: 671.2063\n",
      "Epoch 146/250\n",
      "456/456 [==============================] - 0s 150us/step - loss: 623.8938 - val_loss: 724.0471\n",
      "Epoch 147/250\n",
      "456/456 [==============================] - 0s 132us/step - loss: 598.5447 - val_loss: 719.3395\n",
      "Epoch 148/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 590.7751 - val_loss: 715.5960\n",
      "Epoch 149/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 595.7989 - val_loss: 736.1639\n",
      "Epoch 150/250\n",
      "456/456 [==============================] - 0s 133us/step - loss: 599.0359 - val_loss: 723.9906\n",
      "Epoch 151/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 594.0792 - val_loss: 706.1375\n",
      "Epoch 152/250\n",
      "456/456 [==============================] - 0s 144us/step - loss: 602.3558 - val_loss: 721.4866\n",
      "Epoch 153/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 592.1489 - val_loss: 745.4054\n",
      "Epoch 154/250\n",
      "456/456 [==============================] - 0s 155us/step - loss: 596.1528 - val_loss: 740.3620\n",
      "Epoch 155/250\n",
      "456/456 [==============================] - 0s 168us/step - loss: 581.4651 - val_loss: 737.1778\n",
      "Epoch 156/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 600.1761 - val_loss: 747.8733\n",
      "Epoch 157/250\n",
      "456/456 [==============================] - 0s 143us/step - loss: 616.3278 - val_loss: 711.7630\n",
      "Epoch 158/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 602.6444 - val_loss: 742.3407\n",
      "Epoch 159/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 604.5580 - val_loss: 707.5882\n",
      "Epoch 160/250\n",
      "456/456 [==============================] - 0s 144us/step - loss: 619.9287 - val_loss: 769.8171\n",
      "Epoch 161/250\n",
      "456/456 [==============================] - 0s 143us/step - loss: 618.3306 - val_loss: 726.0007\n",
      "Epoch 162/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 620.4883 - val_loss: 744.7812\n",
      "Epoch 163/250\n",
      "456/456 [==============================] - 0s 144us/step - loss: 586.6386 - val_loss: 722.5937\n",
      "Epoch 164/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 595.9334 - val_loss: 752.5428\n",
      "Epoch 165/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 614.8901 - val_loss: 717.9692\n",
      "Epoch 166/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 606.5366 - val_loss: 733.0594\n",
      "Epoch 167/250\n",
      "456/456 [==============================] - 0s 142us/step - loss: 608.5878 - val_loss: 755.8891\n",
      "Epoch 168/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 598.6795 - val_loss: 756.3487\n",
      "Epoch 169/250\n",
      "456/456 [==============================] - 0s 133us/step - loss: 589.4953 - val_loss: 719.6897\n",
      "Epoch 170/250\n",
      "456/456 [==============================] - 0s 152us/step - loss: 604.0154 - val_loss: 718.3288\n",
      "Epoch 171/250\n",
      "456/456 [==============================] - 0s 171us/step - loss: 589.1989 - val_loss: 704.8409\n",
      "Epoch 172/250\n",
      "456/456 [==============================] - 0s 143us/step - loss: 581.7646 - val_loss: 729.1689\n",
      "Epoch 173/250\n",
      "456/456 [==============================] - 0s 145us/step - loss: 592.8499 - val_loss: 773.9203\n",
      "Epoch 174/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 628.5260 - val_loss: 755.3161\n",
      "Epoch 175/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 582.1776 - val_loss: 753.0790\n",
      "Epoch 176/250\n",
      "456/456 [==============================] - 0s 152us/step - loss: 596.5647 - val_loss: 767.9120\n",
      "Epoch 177/250\n",
      "456/456 [==============================] - 0s 145us/step - loss: 606.3833 - val_loss: 749.6105\n",
      "Epoch 178/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 617.0238 - val_loss: 747.5440\n",
      "Epoch 179/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 607.6000 - val_loss: 759.2048\n",
      "Epoch 180/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 631.3152 - val_loss: 775.9045\n",
      "Epoch 181/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 626.5799 - val_loss: 734.7717\n",
      "Epoch 182/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 600.3046 - val_loss: 750.6629\n",
      "Epoch 183/250\n",
      "456/456 [==============================] - 0s 132us/step - loss: 601.5668 - val_loss: 714.0490\n",
      "Epoch 184/250\n",
      "456/456 [==============================] - 0s 131us/step - loss: 595.8621 - val_loss: 730.6267\n",
      "Epoch 185/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 605.8052 - val_loss: 697.5514\n",
      "Epoch 186/250\n",
      "456/456 [==============================] - 0s 130us/step - loss: 595.5749 - val_loss: 752.4874\n",
      "Epoch 187/250\n",
      "456/456 [==============================] - 0s 178us/step - loss: 590.5553 - val_loss: 733.8705\n",
      "Epoch 188/250\n",
      "456/456 [==============================] - 0s 151us/step - loss: 594.3194 - val_loss: 704.1718\n",
      "Epoch 189/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 574.4055 - val_loss: 732.8278\n",
      "Epoch 190/250\n",
      "456/456 [==============================] - 0s 153us/step - loss: 569.8636 - val_loss: 710.2944\n",
      "Epoch 191/250\n",
      "456/456 [==============================] - 0s 150us/step - loss: 604.1192 - val_loss: 729.5268\n",
      "Epoch 192/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 586.6300 - val_loss: 721.1490\n",
      "Epoch 193/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 595.5527 - val_loss: 731.7729\n",
      "Epoch 194/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 583.5917 - val_loss: 698.0263\n",
      "Epoch 195/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 590.0375 - val_loss: 724.7929\n",
      "Epoch 196/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 605.6801 - val_loss: 690.9347\n",
      "Epoch 197/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 569.5887 - val_loss: 709.9740\n",
      "Epoch 198/250\n",
      "456/456 [==============================] - 0s 128us/step - loss: 593.8131 - val_loss: 705.4763\n",
      "Epoch 199/250\n",
      "456/456 [==============================] - 0s 144us/step - loss: 573.8148 - val_loss: 682.5202\n",
      "Epoch 200/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 613.1486 - val_loss: 715.9805\n",
      "Epoch 201/250\n",
      "456/456 [==============================] - 0s 153us/step - loss: 616.0377 - val_loss: 721.0379\n",
      "Epoch 202/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 604.0377 - val_loss: 746.5093\n",
      "Epoch 203/250\n",
      "456/456 [==============================] - 0s 171us/step - loss: 590.2959 - val_loss: 726.3161\n",
      "Epoch 204/250\n",
      "456/456 [==============================] - 0s 161us/step - loss: 612.2684 - val_loss: 715.3845\n",
      "Epoch 205/250\n",
      "456/456 [==============================] - 0s 157us/step - loss: 593.8197 - val_loss: 725.1007\n",
      "Epoch 206/250\n",
      "456/456 [==============================] - 0s 150us/step - loss: 623.3973 - val_loss: 767.3366\n",
      "Epoch 207/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 630.7636 - val_loss: 719.7822\n",
      "Epoch 208/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 610.7157 - val_loss: 768.6280\n",
      "Epoch 209/250\n",
      "456/456 [==============================] - 0s 132us/step - loss: 620.6998 - val_loss: 722.4682\n",
      "Epoch 210/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 620.8119 - val_loss: 704.9691\n",
      "Epoch 211/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 606.0306 - val_loss: 714.2641\n",
      "Epoch 212/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 606.1794 - val_loss: 760.5406\n",
      "Epoch 213/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 603.1042 - val_loss: 817.9746\n",
      "Epoch 214/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 594.2371 - val_loss: 797.0042\n",
      "Epoch 215/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 616.2436 - val_loss: 763.1087\n",
      "Epoch 216/250\n",
      "456/456 [==============================] - 0s 147us/step - loss: 595.1586 - val_loss: 772.6501\n",
      "Epoch 217/250\n",
      "456/456 [==============================] - 0s 150us/step - loss: 599.6209 - val_loss: 751.9045\n",
      "Epoch 218/250\n",
      "456/456 [==============================] - 0s 154us/step - loss: 618.5151 - val_loss: 838.7102\n",
      "Epoch 219/250\n",
      "456/456 [==============================] - 0s 169us/step - loss: 765.8815 - val_loss: 868.2358\n",
      "Epoch 220/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 688.2309 - val_loss: 790.3795\n",
      "Epoch 221/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 625.7893 - val_loss: 762.4292\n",
      "Epoch 222/250\n",
      "456/456 [==============================] - 0s 135us/step - loss: 615.7972 - val_loss: 747.6273\n",
      "Epoch 223/250\n",
      "456/456 [==============================] - 0s 141us/step - loss: 595.2755 - val_loss: 743.0821\n",
      "Epoch 224/250\n",
      "456/456 [==============================] - 0s 134us/step - loss: 605.9586 - val_loss: 755.6405\n",
      "Epoch 225/250\n",
      "456/456 [==============================] - 0s 142us/step - loss: 580.6716 - val_loss: 737.4594\n",
      "Epoch 226/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 0s 131us/step - loss: 601.9930 - val_loss: 723.9165\n",
      "Epoch 227/250\n",
      "456/456 [==============================] - 0s 145us/step - loss: 591.8079 - val_loss: 681.9189\n",
      "Epoch 228/250\n",
      "456/456 [==============================] - 0s 133us/step - loss: 588.4875 - val_loss: 694.1081\n",
      "Epoch 229/250\n",
      "456/456 [==============================] - 0s 143us/step - loss: 595.9459 - val_loss: 715.6492\n",
      "Epoch 230/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 583.5656 - val_loss: 704.3492\n",
      "Epoch 231/250\n",
      "456/456 [==============================] - 0s 134us/step - loss: 586.0608 - val_loss: 707.0404\n",
      "Epoch 232/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 609.3332 - val_loss: 751.0331\n",
      "Epoch 233/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 591.4503 - val_loss: 720.1773\n",
      "Epoch 234/250\n",
      "456/456 [==============================] - 0s 136us/step - loss: 586.3380 - val_loss: 708.1180\n",
      "Epoch 235/250\n",
      "456/456 [==============================] - 0s 148us/step - loss: 587.8414 - val_loss: 710.2921\n",
      "Epoch 236/250\n",
      "456/456 [==============================] - 0s 163us/step - loss: 592.0375 - val_loss: 718.9756\n",
      "Epoch 237/250\n",
      "456/456 [==============================] - 0s 145us/step - loss: 591.1218 - val_loss: 740.5281\n",
      "Epoch 238/250\n",
      "456/456 [==============================] - 0s 138us/step - loss: 601.0764 - val_loss: 710.5195\n",
      "Epoch 239/250\n",
      "456/456 [==============================] - 0s 127us/step - loss: 594.0110 - val_loss: 752.9997\n",
      "Epoch 240/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 582.7624 - val_loss: 739.4041\n",
      "Epoch 241/250\n",
      "456/456 [==============================] - 0s 140us/step - loss: 590.2339 - val_loss: 768.9368\n",
      "Epoch 242/250\n",
      "456/456 [==============================] - 0s 153us/step - loss: 605.5266 - val_loss: 761.4414\n",
      "Epoch 243/250\n",
      "456/456 [==============================] - 0s 156us/step - loss: 591.9601 - val_loss: 739.1040\n",
      "Epoch 244/250\n",
      "456/456 [==============================] - 0s 142us/step - loss: 571.3472 - val_loss: 709.0823\n",
      "Epoch 245/250\n",
      "456/456 [==============================] - 0s 130us/step - loss: 585.3056 - val_loss: 707.5424\n",
      "Epoch 246/250\n",
      "456/456 [==============================] - 0s 146us/step - loss: 621.5237 - val_loss: 718.9093\n",
      "Epoch 247/250\n",
      "456/456 [==============================] - 0s 149us/step - loss: 583.6562 - val_loss: 755.3150\n",
      "Epoch 248/250\n",
      "456/456 [==============================] - 0s 164us/step - loss: 594.4030 - val_loss: 704.2976\n",
      "Epoch 249/250\n",
      "456/456 [==============================] - 0s 137us/step - loss: 568.9391 - val_loss: 735.8465\n",
      "Epoch 250/250\n",
      "456/456 [==============================] - 0s 139us/step - loss: 621.2181 - val_loss: 703.9182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d62236c470>"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=250, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.061158251874674385"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, def_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703.9182161774387"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, def_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_preds = rec_model.predict(X_test)\n",
    "best_model_residuals = np.array(best_model_preds) - np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(y_test)\n",
    "output['preds'] = best_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['residuals'] = output['preds'] - output['avg_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Model Residuals')\n",
    "sns.distplot(output['residuals']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
